{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression (predicting a continuous value)\n",
    "\n",
    "\n",
    "Weather Conditions in World War Two: \n",
    "\n",
    "1. Is there a relationship between the daily minimum and maximum temperature? \n",
    "2. Can you predict the maximum temperature given the minimum temperature?\n",
    "\n",
    "https://www.kaggle.com/smid80/weatherww2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files from your local file system\n",
    "#from google.colab import files\n",
    "\n",
    "#uploaded = files.upload()\n",
    "\n",
    "#for fn in uploaded.keys():\n",
    "#  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#      name=fn, length=len(uploaded[fn])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install proj-bin libproj-dev libgeos-dev\n",
    "!pip install https://github.com/matplotlib/basemap/archive/v1.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "df_summary = pd.read_csv(\"Summary of Weather.csv\", low_memory=False)\n",
    "df_station = pd.read_csv(\"Weather Station Locations.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Take a quick look at the data structure\n",
    "## Exploring the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_summary.shape)\n",
    "print(df_station.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the data columns of Dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_summary.info())\n",
    "df_summary.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How categorical data is structured:\n",
    "df_summary.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_station.info())\n",
    "print ('--'*30)\n",
    "df_station.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exploring in *df_summary* the number of values of each 159 Stations: \n",
    "print(df_summary.STA.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exploring in df_station the number of values of each 161 Stations: \n",
    "print(df_station.WBAN.value_counts())\n",
    "print(df_station.WBAN.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the name of columns of the dataset of df_summary and df_station \n",
    "df_summary = df_summary.rename(columns={'STA': 'station'})\n",
    "df_station = df_station.rename(columns={'WBAN': 'station'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the both data frames by the station key\n",
    "df = pd.merge(df_summary, df_station, on=\"station\")\n",
    "\n",
    "# Checking  \n",
    "print(df.station.unique().size)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df_summary.shape)\n",
    "print(df_station.shape)\n",
    "print(df.info())\n",
    "print(df.PoorWeather.unique())\n",
    "print(df.TSHDSBRSGF.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the name of columns of the dataset of df_summary and df_station\n",
    "df = df.rename(columns={'STATE/COUNTRY ID': 'country'})\n",
    "df.info()\n",
    "\n",
    "# Counting the 64 distincs countries:\n",
    "df.country.unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Histogram Before clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins=50, figsize=(10,8))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each column with null values\n",
    "\n",
    "print (df.isnull().sum()/df.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns without significants values: \n",
    "#df = df.drop([\"Date\",\"WindGustSpd\",\"DR\",\"SPD\",\"SND\",\"PGT\",\"FT\",\"FB\",\"FTI\",\"ITH\",\"SD3\",\"RHX\",\"RHN\",\"RVG\",\"WTE\",\"PoorWeather\",\"TSHDSBRSGF\"], axis=1)\n",
    "\n",
    "# Removing any column with 70% null values is not going to add any value.\n",
    "cols = [col for col in df.columns if (df[col].isnull().sum()/df.shape[0] * 100 < 70)]\n",
    "df = df[cols]\n",
    "print(df.info())\n",
    "\n",
    "print ('--'*30)\n",
    "\n",
    "# Removing SNF - SnowFall\n",
    "df = df.drop([\"SNF\"], axis=1)\n",
    "\n",
    "# Removing NAME of Station\n",
    "df = df.drop([\"NAME\"], axis=1)\n",
    "\n",
    "# Removing duplicate columns with Fahrenheit and PRCP values: \n",
    "df = df.drop([\"Date\", \"MAX\",\"MIN\",\"MEA\", \"PRCP\", \"LAT\",\"LON\"], axis=1)\n",
    "\n",
    "\n",
    "print (df.isnull().sum()/df.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that an elevation of 9999 means unknown, so we replace by NaN:\n",
    "# So there is 3510 readings without Elevation\n",
    "\n",
    "df.ELEV = pd.to_numeric(df.ELEV, float)\n",
    "\n",
    "df.loc[df.station == 11501]\n",
    "# Replacing by NaN:\n",
    "df.loc[df.ELEV == 9999, \"ELEV\"] = np.nan\n",
    "df.ELEV = df.ELEV.fillna(df.ELEV.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the empty data in Precip\n",
    "df.Precip = pd.to_numeric(df.Precip, float)\n",
    "df.Precip = df.Precip.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the empty data in Snowfall\n",
    "df.Snowfall = pd.to_numeric(df.Snowfall, float)\n",
    "\n",
    "df.Snowfall = df.Snowfall.fillna(0)\n",
    "\n",
    "\n",
    "print (df.isnull().sum()/df.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram After cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins=50, figsize=(10,8))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap \n",
    "# Extract the data we're interested in\n",
    "lat = df['Latitude'].values\n",
    "lon = df['Longitude'].values\n",
    "maxTemp = df['MaxTemp'].values\n",
    "maxTemp_max = max(df['MaxTemp'].unique())\n",
    "maxTemp_min = min(df['MaxTemp'].unique())   \n",
    "# 1. Draw the map background\n",
    "fig = plt.figure(figsize=(18, 12), edgecolor='w')\n",
    "\n",
    "m = Basemap(projection='mill',lon_0=0)\n",
    "\n",
    "m.drawcoastlines()\n",
    "m.drawparallels(np.arange(-180,180,30),labels=[1,0,0,0])\n",
    "m.drawmeridians(np.arange(m.lonmin,m.lonmax+30,60),labels=[0,0,0,1])\n",
    "m.drawcountries(color='gray')\n",
    "\n",
    "# 2. scatter MaxTemp data, with color reflecting \n",
    "m.scatter(lon, lat, latlon=True,\n",
    "          c=maxTemp, cmap='Reds', alpha=0.5)\n",
    "\n",
    "# 3. create colorbar and legend\n",
    "plt.colorbar(label='MaxTemp')\n",
    "plt.clim(maxTemp_max, maxTemp_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting data with details  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data we're interested in\n",
    "lat = df['Latitude'].values\n",
    "lon = df['Longitude'].values\n",
    "maxTemp = df['MaxTemp'].values\n",
    "maxTemp_max = max(df['MaxTemp'].unique())\n",
    "maxTemp_min = min(df['MaxTemp'].unique())   \n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from itertools import chain\n",
    "\n",
    "# 1. Draw the map background          \n",
    "def draw_map(m, scale=0.2):\n",
    "    # draw a shaded-relief image\n",
    "    m.shadedrelief(scale=scale)\n",
    "    #m.etopo()\n",
    "    # lats and longs are returned as a dictionary\n",
    "    lats = m.drawparallels(np.linspace(-90, 90, 13))\n",
    "    lons = m.drawmeridians(np.linspace(-180, 180, 13))\n",
    "\n",
    "    # keys contain the plt.Line2D instances\n",
    "    lat_lines = chain(*(tup[1][0] for tup in lats.items()))\n",
    "    lon_lines = chain(*(tup[1][0] for tup in lons.items()))\n",
    "    all_lines = chain(lat_lines, lon_lines)\n",
    "    \n",
    "    # cycle through these lines and set the desired style\n",
    "    for line in all_lines:\n",
    "        line.set(linestyle='-', alpha=0.3, color='w')\n",
    "\n",
    "fig = plt.figure(figsize=(18, 7), edgecolor='w')\n",
    "\n",
    "m = Basemap(projection='cyl', resolution=None,\n",
    "            llcrnrlat=-90, urcrnrlat=90,\n",
    "            llcrnrlon=-180, urcrnrlon=180, )\n",
    "# 2. scatter MaxTemp data, with color reflecting \n",
    "m.scatter(lon, lat, latlon=True,\n",
    "          c=maxTemp, cmap='Reds', alpha=0.5)\n",
    "# 3. create colorbar and legend\n",
    "plt.colorbar(label='MaxTemp')\n",
    "plt.clim(maxTemp_max, maxTemp_min)\n",
    "draw_map(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Looking for Correlations \n",
    "\n",
    "\n",
    " 1- Is there a relationship between the daily minimum and maximum temperature?\n",
    " \n",
    " 2- Can you predict the maximum temperature given the minimum temperature?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_matrix[\"MaxTemp\"].sort_values(ascending=False)\n",
    "corr_matrix[\"MinTemp\"].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting correlation Matrix\n",
    "import seaborn as sns\n",
    "sns.heatmap(df.corr(), \n",
    "            annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare the Data for Machine Learning Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df, \n",
    "                                       test_size=0.2, \n",
    "                                       random_state=35)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)   #It's good practice to check\n",
    "\n",
    "train_set.info()\n",
    "\n",
    "print(\"data has {} instances\\n {} train instances\\n {} test intances\".\n",
    "      format(len(df),len(train_set),len(test_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop creates a copy of the remain data and does not affect train_set\n",
    "# Drop the station key and the Day with low correlation\n",
    "train_X = train_set.drop([\"station\",\"DA\",\"MaxTemp\"], axis=1)\n",
    "\n",
    "# copy the label (y) from train_set\n",
    "train_y = train_set.MaxTemp.copy()\n",
    "train_X.describe()\n",
    "train_X.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you need to create an Imputer instance, specifying that you want \n",
    "# to replace each attribute’s missing values with the median of that attribute:\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "\n",
    "# Since the median can only be computed on numerical attributes, we need to \n",
    "# create a copy of the data without the text attribute country:\n",
    "train_X_num = train_X.drop([\"country\"], axis=1)\n",
    "\n",
    "# Now you can fit the imputer instance to the training data using \n",
    "# the fit() method:\n",
    "imputer.fit(train_X_num)\n",
    "\n",
    "imputer.statistics_\n",
    "\n",
    "train_X_num.median().values\n",
    "\n",
    "# Now you can use this “trained” imputer to transform the training set by \n",
    "# replacing missing values by the learned medians:\n",
    "train_X_num_array = imputer.transform(train_X_num)\n",
    "\n",
    "# The result is a plain Numpy array containing the transformed features. \n",
    "# If you want to put it back into a Pandas DataFrame, it’s simple:\n",
    "train_X_num_df = pd.DataFrame(train_X_num_array, columns=train_X_num.columns)\n",
    "\n",
    "train_X_num_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For this, we can use Pandas' factorize() method which maps each \n",
    "# category to a different integer:\n",
    "\n",
    "train_X.country.unique().size\n",
    "\n",
    "train_X_cat_encoded, train_X_categories = train_X.country.factorize()\n",
    "\n",
    "train_X_categories.size\n",
    "# train_X_cat_encoded is now purely numerical\n",
    "train_X_cat_encoded\n",
    "train_X_cat_encoded.size\n",
    "# Scikit-Learn provides a OneHotEncoder encoder to convert \n",
    "# integer categorical values into one-hot vectors.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Numpy's reshape() allows one dimension to be -1, which means \"unspecified\":\n",
    "# the value is inferred from the lenght of the array and the remaining\n",
    "# dimensions\n",
    "train_X_cat_1hot = encoder.fit_transform(train_X_cat_encoded.reshape(-1,1))\n",
    "\n",
    "# it is a column vector\n",
    "train_X_cat_1hot\n",
    "\n",
    "train_X_cat_1hot.toarray().shape\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"Using a sparse matrix: {} bytes\".format(sys.getsizeof(train_X_cat_1hot.toarray())))\n",
    "print(\"Using a dense numpy array: {} bytes\".format(sys.getsizeof(train_X_cat_1hot)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', Imputer(strategy=\"median\")),\n",
    "                         ('std_scaler', StandardScaler())\n",
    "                        ])\n",
    "train_X_num_pipeline = num_pipeline.fit_transform(train_X_num)\n",
    "\n",
    "train_X_num_pipeline\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# This class will transform the data by selecting the desired attributes,\n",
    "# dropping the rest, and converting the resulting DataFrame to a NumPy array.\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, attribute_names):\n",
    "    self.attribute_names = attribute_names\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    return X[self.attribute_names].values\n",
    "\n",
    "\n",
    "# Used to join two or more pipelines into a single pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/10521\n",
    "from future_encoders import OneHotEncoder\n",
    "\n",
    "# numerical columns \n",
    "num_attribs = list(train_X_num.columns)\n",
    "\n",
    "# categorical columns\n",
    "cat_attribs = [\"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical columns\n",
    "num_pipeline = Pipeline([('selector', DataFrameSelector(num_attribs)),\n",
    "                         ('imputer', Imputer(strategy=\"mean\")),\n",
    "                         ('std_scaler', StandardScaler())\n",
    "                        ])\n",
    "\n",
    "# pipeline for categorical column\n",
    "cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attribs)),\n",
    "                         ('cat_encoder', OneHotEncoder(sparse=False))\n",
    "                        ])\n",
    "\n",
    "# a full pipeline handling both numerical and categorical attributes\n",
    "full_pipeline = FeatureUnion(transformer_list=[(\"num_pipeline\", num_pipeline),\n",
    "                                               (\"cat_pipeline\", cat_pipeline)\n",
    "                                              ])\n",
    "    \n",
    "    \n",
    "# you can run the whole pipeline simply\n",
    "train_X_prepared = full_pipeline.fit_transform(train_X)\n",
    "train_X_prepared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and Train a Model with Linear Regression:    \n",
    "    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a LinearRegression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# fit it\n",
    "lin_reg.fit(train_X_prepared, train_y)\n",
    "\n",
    "\n",
    "# Done!! You now have a working Linear Regression Model.\n",
    "# Let's try it out on a few instances from the trainning set.\n",
    "\n",
    "# prepare the data\n",
    "some_data = train_X.iloc[:10]\n",
    "some_labels = train_y.iloc[:10]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "# make predictions\n",
    "print(\"Predictions:\\n\", lin_reg.predict(some_data_prepared)) \n",
    "\n",
    "# Compare against the actual values:\n",
    "print(\"Labels:\\n\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "maxTemp_predictions = lin_reg.predict(train_X_prepared)\n",
    "lin_mse = mean_squared_error(train_y, maxTemp_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(train_X_prepared, train_y)\n",
    "\n",
    "\n",
    "# now that the model is trained, let's evaluate it on the training set\n",
    "\n",
    "maxTemp_predictions = tree_reg.predict(some_data_prepared)\n",
    "tree_mse = mean_squared_error(some_labels, maxTemp_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "\n",
    "# Out[]: 0.23215133577685879\n",
    "# !!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# create a LinearRegression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(lin_reg, \n",
    "                         train_X_prepared, \n",
    "                         train_y,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "rmse_scores\n",
    "\n",
    "def display_scores(scores):\n",
    "  print(\"Scores:\", scores)\n",
    "  print(\"Mean:\", scores.mean())\n",
    "  print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(rmse_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning with RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create a RandomForestRegressor model\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "# fit it\n",
    "forest_reg.fit(train_X_prepared, train_y)\n",
    "\n",
    "# predict the prepared data\n",
    "maxTemp_predictions = forest_reg.predict(train_X_prepared)\n",
    "\n",
    "forest_mse = mean_squared_error(train_y, maxTemp_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "forest_scores = cross_val_score(forest_reg,\n",
    "                                train_X_prepared, \n",
    "                                train_y,\n",
    "                                scoring=\"neg_mean_squared_error\", \n",
    "                                cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With linear Regression\n",
    "from sklearn.linear_model import Lasso, Ridge, SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# hyperparameters values\n",
    "param_grid = [\n",
    "    {\n",
    "        'regr': [Lasso(), Ridge()],\n",
    "        'regr__alpha': np.logspace(-4, 1, 6),\n",
    "    },\n",
    "    {\n",
    "        'regr': [SGDRegressor()],\n",
    "        'regr__alpha': np.logspace(-5, 0, 6),\n",
    "        'regr__max_iter': [500, 1000],\n",
    "    },\n",
    "]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('regr', Lasso())\n",
    "])\n",
    "\n",
    "# create a randomforeestregressor model\n",
    "#lin_reg = LinearRegression()\n",
    "\n",
    "\n",
    "# run the grid search with cross validation\n",
    "grid_search_LR = GridSearchCV(pipe, \n",
    "                           param_grid, \n",
    "                           cv=5,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "# see 90 combinations!!!\n",
    "# it may take quite a long time\n",
    "grid_search_LR.fit(train_X_prepared, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_LR.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_LR.best_estimator_\n",
    "\n",
    "#Out[]: \n",
    "#Pipeline(memory=None,\n",
    "#     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), \n",
    "#     ('regr', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "#   normalize=False, random_state=None, solver='auto', tol=0.001))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and of course, the evaluation scores are also available\n",
    "cvres = grid_search_LR.cv_results_\n",
    "cvres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model found in gridsearch step of Linear Regression:\n",
    "model_LR = grid_search_LR.best_estimator_\n",
    "\n",
    "# predictors and label\n",
    "test_X = test_set.drop([\"station\",\"MaxTemp\"], axis=1)\n",
    "test_y = test_set[\"MaxTemp\"].copy()\n",
    "\n",
    "# prepared test's predictors\n",
    "test_X_prepared = full_pipeline.transform(test_X)\n",
    "\n",
    "predictions = model_LR.predict(test_X_prepared)\n",
    "mse = mean_squared_error(test_y, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# hyperparameters values\n",
    "# param_grid[0] - 12 combinations\n",
    "# param_grid[1] - 6 combinations\n",
    "param_grid = [{'n_estimators': [3, 10, 30], \n",
    "               'max_features': [2, 4, 6, 8, 10]\n",
    "              },\n",
    "              {'bootstrap': [False], \n",
    "               'n_estimators': [3, 10],\n",
    "               'max_features': [2, 3, 4]\n",
    "              }\n",
    "             ]\n",
    "\n",
    "# create a randomforeestregressor model\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "\n",
    "# run the grid search with cross validation\n",
    "# (12 + 6) x 5 = 90 combinations\n",
    "grid_search_RF = GridSearchCV(forest_reg, \n",
    "                           param_grid, \n",
    "                           cv=5,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "# see 90 combinations!!!\n",
    "# it may take quite a long time\n",
    "grid_search_RF.fit(train_X_prepared, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_RF.best_params_\n",
    "#Out[42]: {'max_features': 8, 'n_estimators': 30}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_RF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and of course, the evaluation scores are also available\n",
    "cvres = grid_search_RF.cv_results_\n",
    "cvres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model found in gridsearch step\n",
    "model_RF = grid_search_RF.best_estimator_\n",
    "\n",
    "# predictors and label\n",
    "test_X = test_set.drop([\"station\",\"MaxTemp\"], axis=1)\n",
    "test_y = test_set[\"MaxTemp\"].copy()\n",
    "\n",
    "# prepared test's predictors\n",
    "test_X_prepared = full_pipeline.transform(test_X)\n",
    "\n",
    "\n",
    "predictions = model_RF.predict(test_X_prepared)\n",
    "mse = mean_squared_error(test_y, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n",
    "# 0.902328423775"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Analyze the Best Models and Their Errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can indicate the relative importance of each attribute \n",
    "# for making accurate predictions for Random Forest\n",
    "feature_importances_RF = grid_search_RF.best_estimator_.feature_importances_\n",
    "feature_importances_RF\n",
    "\n",
    "\n",
    "# categorical component of pipeline\n",
    "#cat_encoder = cat_pipeline.named_steps[\"country\"]\n",
    "\n",
    "# get the names\n",
    "#cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "\n",
    "# all columns names\n",
    "attributes = num_attribs \n",
    "\n",
    "sorted(zip(feature_importances_RF, num_attribs), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Presenting my solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

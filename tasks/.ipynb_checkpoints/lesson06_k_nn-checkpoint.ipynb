{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xwkIPyqwMt0"
   },
   "source": [
    "# Upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "ILce_K6dwiyE",
    "outputId": "dae0f64d-9659-4362-d5ee-d2aeaa47cfd5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-6ec43994-1248-4060-82c4-a134456a578b\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-6ec43994-1248-4060-82c4-a134456a578b\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving listings.csv.gz to listings.csv.gz\n",
      "User uploaded file \"listings.csv.gz\" with length 29277276 bytes\n"
     ]
    }
   ],
   "source": [
    "# Uploading files from your local file system\n",
    "# AmesHousing.txt\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "#wget https://www.dropbox.com/s/l46kqgcqfnlaqx1/listings.csv.gz?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iopwJwTQterS"
   },
   "outputs": [],
   "source": [
    "!gunzip 'listings.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "re0FjGikuytU",
    "outputId": "0509ee30-7f1f-4e47-e413-c90db24b40ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 112M\n",
      "-rw-r--r-- 1 root root 112M Oct  5 13:18 listings.csv\n",
      "drwxr-xr-x 2 root root   1M Sep 28 23:32 sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wUEFZSvBtxy"
   },
   "source": [
    "# 1 - Introduction to K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmUKBpTl_iDw"
   },
   "source": [
    "## 1.1 - Problem definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61VrJ52m0tg7"
   },
   "source": [
    "\n",
    "\n",
    "**AirBnB** is a marketplace for short term rentals that allows you to list part or all of your living space for others to rent. You can rent everything from a room in an apartment to your entire house on AirBnB. Because most of the listings are on a short-term basis, AirBnB has grown to become a popular alternative to hotels. The company itself has grown from it's founding in 2008 to a 30 billion dollar [valuation in 2016](http://www.bloomberg.com/news/articles/2016-08-05/airbnb-files-to-raise-850-million-at-30-billion-valuation) and is currently worth more than any hotel chain in the world.\n",
    "\n",
    "One challenge that hosts looking to rent their living space face is determining the optimal nightly rent price. In many areas, renters are presented with a good selection of listings and can filter on criteria like price, number of bedrooms, room type and more. Since AirBnB is a marketplace, the amount a host can charge on a nightly basis is closely linked to the dynamics of the marketplace. Here's a screenshot of the search experience on AirBnB:\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1pJqCJLih-jjSDtTF1m2j2bY8O_dGrj9M\">\n",
    "\n",
    "As a host, if we try to charge above market price for a living space we'd like to rent, then renters will select more affordable alternatives which are similar to ours.. If we set our nightly rent price too low, we'll miss out on potential revenue.\n",
    "\n",
    "One strategy we could use is to:\n",
    "\n",
    "- find a few listings that are similar to ours,\n",
    "- average the listed price for the ones most similar to ours,\n",
    "- set our listing price to this calculated average price.\n",
    "\n",
    "The process of discovering patterns in existing data to make a prediction is called **machine learning**. In our case, we want to use data on local listings to predict the optimal price for us to set. In this lesson, we'll explore a specific machine learning technique called **k-nearest neighbors**, which mirrors the strategy we just described. Before we dive further into machine learning and k-nearest neighbors, let's get familiar with the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkhhRIDa_iDx"
   },
   "source": [
    "## 1.2 - Introduction to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngmkKQh40piE"
   },
   "source": [
    "\n",
    "\n",
    "While AirBnB doesn't release any data on the listings in their marketplace, a separate group named [Inside AirBnB](http://insideairbnb.com/get-the-data.html) has extracted data on a sample of the listings for many of the major cities on the website. In this lesson, we'll be working with their dataset from **September 14, 2018** on the listings from **Rio de Janeiro -Brazil**, the capital of the Samba. Here's a [direct link to that dataset](http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2018-09-14/data/listings.csv.gz). Each row in the dataset is a specific listing that's available for renting on AirBnB in the Rio de Janeiro.\n",
    "\n",
    "To make the dataset less cumbersome to work with, we will removed many of the columns in the original dataset (96 columns). Here are the columns we will keep:\n",
    "\n",
    "- **host_response_rate**: the response rate of the host\n",
    "- **host_acceptance_rate**: number of requests to the host that convert to rentals\n",
    "- **host_listings_count**: number of other listings the host has\n",
    "- **latitude**: latitude dimension of the geographic coordinates\n",
    "- **longitude**: longitude part of the coordinates\n",
    "- **city**: the city the living space resides\n",
    "- **zipcode**: the zip code the living space resides\n",
    "- **state**: the state the living space resides\n",
    "- **accommodates**: the number of guests the rental can accommodate\n",
    "- **room_type**: the type of living space (Private room, Shared room or Entire home/apt\n",
    "- **bedrooms**: number of bedrooms included in the rental\n",
    "- **bathrooms**: number of bathrooms included in the rental\n",
    "- **beds**: number of beds included in the rental\n",
    "- **price**: nightly price for the rental\n",
    "- **cleaning_fee**: additional fee used for cleaning the living space after the guest leaves\n",
    "- **security_deposit**: refundable security deposit, in case of damages\n",
    "- **minimum_nights**: minimum number of nights a guest can stay for the rental\n",
    "- **maximum_nights**: maximum number of nights a guest can stay for the rental\n",
    "- **number_of_reviews**: number of reviews that previous guests have left\n",
    "\n",
    "Let's read the dataset into Pandas and become more familiar with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "TKfZj-kKv9t9",
    "outputId": "bd1ba35d-16ef-49bc-c705-ff1d8163beb2"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <th>host_listings_count</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>price</th>\n",
       "      <th>security_deposit</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>maximum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Copacabana</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>22071-020</td>\n",
       "      <td>-22.978083</td>\n",
       "      <td>-43.190977</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>$179.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>22071-100</td>\n",
       "      <td>-22.981783</td>\n",
       "      <td>-43.193041</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>$150.00</td>\n",
       "      <td>$330.00</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>22081-020</td>\n",
       "      <td>-22.983729</td>\n",
       "      <td>-43.192207</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>$250.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1125</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Copacabana</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>22070-001</td>\n",
       "      <td>-22.983367</td>\n",
       "      <td>-43.190687</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>$150.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$100.00</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>22061-020</td>\n",
       "      <td>-22.976700</td>\n",
       "      <td>-43.192468</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>$50.00</td>\n",
       "      <td>$1,500.00</td>\n",
       "      <td>$150.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  host_response_rate  host_acceptance_rate  host_listings_count  \\\n",
       "0               100%                   NaN                  3.0   \n",
       "1               100%                   NaN                 12.0   \n",
       "2                70%                   NaN                  1.0   \n",
       "3               100%                   NaN                  1.0   \n",
       "4                92%                   NaN                  2.0   \n",
       "\n",
       "             city           state    zipcode   latitude  longitude  \\\n",
       "0      Copacabana  Rio de Janeiro  22071-020 -22.978083 -43.190977   \n",
       "1  Rio de Janeiro  Rio de Janeiro  22071-100 -22.981783 -43.193041   \n",
       "2  Rio de Janeiro  Rio de Janeiro  22081-020 -22.983729 -43.192207   \n",
       "3      Copacabana  Rio de Janeiro  22070-001 -22.983367 -43.190687   \n",
       "4  Rio de Janeiro  Rio de Janeiro  22061-020 -22.976700 -43.192468   \n",
       "\n",
       "         room_type  accommodates  bathrooms  bedrooms  beds    price  \\\n",
       "0  Entire home/apt             4        1.0       1.0   2.0  $179.00   \n",
       "1  Entire home/apt             4        1.0       0.0   1.0  $150.00   \n",
       "2  Entire home/apt             5        3.0       2.0   2.0  $250.00   \n",
       "3  Entire home/apt             4        1.0       0.0   2.0  $150.00   \n",
       "4  Entire home/apt             4        1.0       1.0   3.0   $50.00   \n",
       "\n",
       "  security_deposit cleaning_fee  minimum_nights  maximum_nights  \\\n",
       "0              NaN      $120.00               3              45   \n",
       "1          $330.00      $120.00               2            1125   \n",
       "2            $0.00      $120.00               1            1125   \n",
       "3            $0.00      $100.00               2              14   \n",
       "4        $1,500.00      $150.00               3            1125   \n",
       "\n",
       "   number_of_reviews  \n",
       "0                  6  \n",
       "1                  1  \n",
       "2                 45  \n",
       "3                  0  \n",
       "4                  1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target_columns = [\"host_response_rate\", \"host_acceptance_rate\", \"host_listings_count\",\n",
    "                  \"latitude\",\"longitude\",\"city\",\"zipcode\",\"state\",\"accommodates\",\n",
    "                  \"room_type\",\"bedrooms\",\"bathrooms\",\"beds\",\"price\",\"cleaning_fee\",\n",
    "                  \"security_deposit\",\"minimum_nights\",\"maximum_nights\",\"number_of_reviews\"]\n",
    "\n",
    "rio_listings = pd.read_csv(\"listings.csv\",usecols=target_columns,low_memory=False)\n",
    "\n",
    "rio_listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk8-8iIH_iDy"
   },
   "source": [
    "## 1.3 -  K-nearest neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gn_hvm60y1I"
   },
   "source": [
    "\n",
    "\n",
    "Here's the strategy we wanted to use:\n",
    "\n",
    "- Find a few similar listings.\n",
    "- Calculate the average nightly rental price of these listings.\n",
    "- Set the average price as the price for our listing.\n",
    "\n",
    "The k-nearest neighbors algorithm is similar to this strategy. Here's an overview:\n",
    "\n",
    "<img width=\"900\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1b3uN7WtvbamsIVxYYML1cXbWTXvs6UGb\">\n",
    "\n",
    "\n",
    "There are 2 things we need to unpack in more detail:\n",
    "\n",
    "- the similarity metric\n",
    "- how to choose the **k** value\n",
    "\n",
    "In this section, we'll define what similarity metric we're going to use. Then, we'll implement the **k-nearest neighbors** algorithm and use it to suggest a price for a new, unpriced listing. We'll use a **k** value of 5 in this section. In later sections, we'll learn how to evaluate how good the suggested prices are, how to choose the optimal **k** value, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtnKXaWZ_iDz"
   },
   "source": [
    "## 1.4 Euclidean distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBYFixkn8hzX"
   },
   "source": [
    "\n",
    "\n",
    "The similarity metric works by comparing a fixed set of numerical **features**, another word for attributes, between 2 **observations**, or living spaces in our case. When trying to predict a continuous value, like price, the main similarity metric that's used is **Euclidean distance**. Here's the general formula for Euclidean distance:\n",
    "\n",
    "$\\displaystyle d = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + \\ldots + (q_n - p_n)^2}$\n",
    "\n",
    "where $q_1$ to $q_n$ represent the feature values for one observation and $p_1$ to $p_n$ represent the feature values for the other observation. Here's a diagram that breaks down the Euclidean distance between the first 2 observations in the dataset using only the **host_listings_count**, **accommodates**, **bedrooms**, **bathrooms**, and **beds** columns:\n",
    "\n",
    "\n",
    "<img width=\"900\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=15wH6nSdX74TEKIFeBqNrMceiwoCk5j3s\">\n",
    "\n",
    "In this mission, we'll use just one feature in this mission to keep things simple as you become familiar with the machine learning workflow. Since we're only using one feature, this is known as the **univariate case**. Here's how the formula looks like for the univariate case:\n",
    "\n",
    "$\\displaystyle d = \\sqrt{(q_1 - p_1)^2}$\n",
    "\n",
    "The square root and the squared power cancel and the formula simplifies to:\n",
    "\n",
    "$ \\displaystyle d = \\left | q_1 - p_1 \\right |$\n",
    "\n",
    "The living space that we want to rent can accommodate 3 people. Let's first calculate the distance, using just the **accommodates** feature, between the first living space in the dataset and our own.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    "\n",
    "\n",
    "\n",
    "1. Calculate the Euclidean distance between our living space, which can accommodate 3 people, and the first living space in the **rio_listings** Dataframe. For the sake of understanding, use only the column \"accommodates\".\n",
    "2. Assign the result to **first_distance** and display the value using the **print** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ebUrbW_Z96Qp",
    "outputId": "4c92d819-29d5-4d8e-8746-28941685188a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# put your code here\n",
    "#rio_listings[rio_listings.accommodates == 3].head(1) \n",
    "first_distance = abs(3 - rio_listings[\"accommodates\"][0])\n",
    "print(first_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4HoOhDH_iD0"
   },
   "source": [
    "## 1.5. Calculate distance for all observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lys4UsVp-d2I"
   },
   "source": [
    "\n",
    "\n",
    "The Euclidean distance between the first row in the **rio_listings** Dataframe and our own living space is **1**. How do we know if this is high or low? If you look at the Euclidean distance equation itself, the lowest value you can achieve is **0**. This happens when the value for the feature is exactly the same for both observations you're comparing. If $p_1=q_1$, then $ \\displaystyle d = \\left | q_1 - p_1 \\right |$ which results in $d=0$. The closer to **0** the distance the more similar the living spaces are.\n",
    "\n",
    "If we wanted to calculate the Euclidean distance between each living space in the dataset and a living space that accommodates **8** people, here's a preview of what that would look like.\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1res4uO-8wP8_g7scMbr1kP594ZnOmk-y\">\n",
    "\n",
    "Then, we can rank the existing living spaces by ascending distance values, the proxy for similarity.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Calculate the distance between each value in the **accommodates** column from **rio_listings** and the value **3**, which is the number of people our listing accommodates:\n",
    "    - Use the [apply](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) method to calculate the absolute value between each value in **accommodates** and **3** and return a new Series containing the distance values.\n",
    "2. Assign the distance values to the **distance** column.\n",
    "3. Use the Series method **value_counts** and the **print** function to display the unique value counts for the **distance** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "JsGX1QQ1Atll",
    "outputId": "b9ec2907-5170-44cd-a6f6-928fb26d6733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      19102\n",
      "2       5283\n",
      "3       5078\n",
      "0       3575\n",
      "5       1511\n",
      "4        840\n",
      "7        779\n",
      "9        296\n",
      "13       266\n",
      "6        249\n",
      "12        86\n",
      "11        76\n",
      "8         60\n",
      "10        43\n",
      "157        1\n",
      "Name: accommodates, dtype: int64\n",
      "========\n",
      "4      9579\n",
      "2      9523\n",
      "6      5078\n",
      "3      3575\n",
      "5      3125\n",
      "1      2158\n",
      "8      1511\n",
      "7       840\n",
      "10      779\n",
      "12      296\n",
      "16      266\n",
      "9       249\n",
      "15       86\n",
      "14       76\n",
      "11       60\n",
      "13       43\n",
      "160       1\n",
      "Name: accommodates, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# put your code here\n",
    "\n",
    "\n",
    "rio_listings['distance'] = rio_listings['accommodates'].apply(lambda x: abs(3 - x))\n",
    "print(distance.value_counts())\n",
    "print(\"========\")\n",
    "print(rio_listings['accommodates'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaePLCMj_iD0"
   },
   "source": [
    "## 1.6 -  Randomizing, and sorting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tbf-vrFncV4"
   },
   "source": [
    "\n",
    "\n",
    "It looks like there are quite a few, 3575 to be precise, living spaces that can accommodate 3 people just like ours. This means the 5 \"nearest neighbors\" we select after sorting all will have a distance value of 0. If we sort by the **distance** column and then just select the first 5 living spaces, we would be **biasing** the result to the ordering of the dataset.\n",
    "\n",
    "```python\n",
    "dc_listings[dc_listings[\"distance\"] == 0][\"accommodates\"]\n",
    "26      3\n",
    "34      3\n",
    "36      3\n",
    "40      3\n",
    "44      3\n",
    "45      3\n",
    "48      3\n",
    "65      3\n",
    "66      3\n",
    "71      3\n",
    "75      3\n",
    "86      3\n",
    "...\n",
    "```\n",
    "\n",
    "Let's instead randomize the ordering of the dataset and then sort the Dataframe by the **distance** column. This way, all of the living spaces with the same number of bedrooms will still be at the top of the Dataframe but will be in random order across the first 3575 rows. We've already done the first step of setting the random seed, so we can perform answer checking on our end.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    "\n",
    "\n",
    "\n",
    "1. Randomize the order of the rows in **rio_listings**:\n",
    "    - Use the **np.random.permutation()** function to return a NumPy array of shuffled index values.\n",
    "    - Use the Dataframe method **loc[]** to return a new Dataframe containing the shuffled order.\n",
    "    - Assign the new Dataframe back to **rio_listings**.\n",
    "2. After randomization, sort **rio_listings** by the **distance** column.\n",
    "3. Display the first 10 values in the **price** column using the **print** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "ekW6ePVr_iD1",
    "outputId": "f6fd1ece-81fc-40f9-c87c-3a8f4d0e7479"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c0e2d7422e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrio_listings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrio_listings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrio_listings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# put your code here\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "rio_listings = np.random.permutation(rio_listings.loc[:,:])\n",
    "print(rio_listings['distance'].sort_values().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23Q2awTs_iD4"
   },
   "source": [
    "## 1.7 -  Average price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXJKMZjSSh2W"
   },
   "source": [
    "\n",
    "\n",
    "Before we can select the 5 most similar living spaces and compute the average price, we need to clean the **price** column. Right now, the **price** column contains comma characters (**,**) and dollar sign characters and is formatted as a text column instead of a numeric one. We need to remove these values and convert the entire column to the float datatype. Then, we can calculate the average price.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    "\n",
    "\n",
    "\n",
    "1. Remove the commas (**,**) and dollar sign characters (**$**) from the **price** column:\n",
    "    - Use the **str** accessor so we can apply string methods to each value in the column followed by the string method replace to replace all comma characters with the empty character: **stripped_commas = rio_listings['price'].str.replace(',', '')**\n",
    "    - Repeat to remove the dollar sign characters as well.\n",
    "2. Convert the new Series object containing the cleaned values to the **float** datatype and assign back to the **price** column in **rio_listings**.\n",
    "3. Calculate the mean of the first 5 values in the **price** column and assign to **mean_price**.\n",
    "4. Use the **print** function or the variable inspector below to display **mean_price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8shEFEJTvLP"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVJxA-wk_iD5"
   },
   "source": [
    "## 1.8 Function to make predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hD4OHD4Uc9R"
   },
   "source": [
    "\n",
    "Congrats! You've just made your first prediction! Based on the average price of other listings that accommdate 3 people, we should charge **357.09** dollars per night for a guest to stay at our living space. In the next section, we'll dive into evaluating how good of a prediction this is.\n",
    "\n",
    "Let's write a more general function that can suggest the optimal price for other values of the **accommodates** column. The **rio_listings** Dataframe has information specific to our living space, e.g. the **distance** column. To save you time, we've reset the **rio_listings** Dataframe to a clean state and only kept the data cleaning and randomization we did since those weren't unique to the prediction we were making for our living space.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    "\n",
    "\n",
    "1. Write a function named **predict_price** that can use the **k-nearest neighbors machine learning** technique to calculate the suggested price for any value for **accommodates**. This function should:\n",
    "    - Take in a single parameter, **new_listing**, that describes the **number of bedrooms**.\n",
    "    - Assign **rio_listings** to a new Dataframe named **temp_df** so we aren't constantly modifying the original dataset each time we call the function.\n",
    "    - Calculate the distance between each value in the **accommodates** column and the **new_listing** value that was passed in. Assign the resulting Series object to the **distance** column in **temp_df**.\n",
    "    - Sort **temp_df** by the **distance** column and select the first 5 values in the **price** column. Don't randomize the ordering of **temp_df**.\n",
    "    - Calculate the mean of these 5 values and use that as the return value for the entire **predict_price** function.\n",
    "2. Use the **predict_price** function to suggest a price for a living space that:\n",
    "    - accommodates 1 person, assign the suggested price to **acc_one**.\n",
    "    - accommodates 2 people, assign the suggested price to **acc_two**.\n",
    "    - accommodates 4 people, assign the suggested price to **acc_four**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gEl2Mmr_iD6"
   },
   "outputs": [],
   "source": [
    "# Brought along the changes we made to the 'rio_listings' Dataframe.\n",
    "target_columns = [\"host_response_rate\", \"host_acceptance_rate\", \"host_listings_count\",\n",
    "                  \"latitude\",\"longitude\",\"city\",\"zipcode\",\"state\",\"accommodates\",\n",
    "                  \"room_type\",\"bedrooms\",\"bathrooms\",\"beds\",\"price\",\"cleaning_fee\",\n",
    "                  \"security_deposit\",\"minimum_nights\",\"maximum_nights\",\"number_of_reviews\"]\n",
    "\n",
    "rio_listings = pd.read_csv(\"listings.csv\",usecols=target_columns,low_memory=False)\n",
    "\n",
    "stripped_commas = rio_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "rio_listings['price'] = stripped_dollars.astype('float')\n",
    "rio_listings = rio_listings.loc[np.random.permutation(len(rio_listings))]\n",
    "\n",
    "def predict_price(new_listing):\n",
    "    ## Complete the function.\n",
    "    ## Put your code here\n",
    "    #\n",
    "    #\n",
    "    return(new_listing)\n",
    "\n",
    "acc_one = predict_price(1)\n",
    "acc_two = predict_price(2)\n",
    "acc_four = predict_price(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcJp4JUT_iD9"
   },
   "source": [
    "## 1.9 - Next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhG6VfdGi5Ka"
   },
   "source": [
    "\n",
    "\n",
    "In this section, we explored the problem of predicting the optimal price to list an AirBnB rental for based on the price of similar listings on the site. We stepped through the entire machine learning workflow, from selecting a feature to testing the model. To explore the basics of machine learning, we limited ourselves to only using one feature (the univariate case) and a fixed **k** value.\n",
    "\n",
    "In the next section, we'll learn how to evaluate a model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SeblUeJjIZO"
   },
   "source": [
    "# 2 -  Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OpXKHwKWkPFe"
   },
   "source": [
    "## 2.1 - Testing quality of predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hcgc64IHy10x"
   },
   "source": [
    "\n",
    "\n",
    "We now have a function that can predict the price for any living space we want to list as long as we know the number of people it can accommodate. The function we wrote represents a **machine learning model**, which means that it outputs a prediction based on the input to the model.\n",
    "\n",
    "A simple way to test the quality of your model is to:\n",
    "\n",
    "- split the dataset into 2 partitions:\n",
    "    - the **training set**: contains the majority of the rows (75%)\n",
    "    - the **test set**: contains the remaining minority of the rows (25%)\n",
    "- use the rows in the training set to predict the **price** value for the rows in the test set\n",
    "    - add new column named **predicted_price** to the test set\n",
    "- compare the **predicted_price** values with the actual  **price** values in the test set to see how accurate the predicted values were.\n",
    "\n",
    "This validation process, where we use the training set to make predictions and the test set to predict values for, is known as **train/test validation**. Whenever you're performing machine learning, you want to perform validation of some kind to ensure that your machine learning model can make good predictions on new data. While train/test validation isn't perfect, we'll use it to understand the validation process, to select an error metric, and then we'll dive into a more robust validation process later in this lesson.\n",
    "\n",
    "Let's modify the **predicted_price** function to use only the rows in the training set, instead of the full dataset, to find the nearest neighbors, average the **price** values for those rows, and return the predicted price value. Then, we'll use this function to predict the price for just the rows in the test set. Once we have the predicted price values, we can compare with the true price values and start to understand the model's effectiveness in the next screen.\n",
    "\n",
    "To start, we've gone ahead and assigned the first 75% of the rows in **rio_listings** to **train_df** and the last 25% of the rows to **test_df**. Here's a diagram explaining the split:\n",
    "\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1H3_0jjS2n8YeVzshom8if0-gB7FVP8WL\">\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Within the **predict_price** function, change the Dataframe that **temp_df** is assigned to. Change it from **rio_listings** to **train_df**, so only the training set is used.\n",
    "2. Use the Series method **apply** to pass all of the values in the **accommodates** column from **test_df** through the **predict_price** function.\n",
    "3. Assign the resulting Series object to the **predict_price** column in **test_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVoRXGv6kPFf"
   },
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Brought along the changes we made to the 'rio_listings' Dataframe.\n",
    "target_columns = [\"host_response_rate\", \"host_acceptance_rate\", \"host_listings_count\",\n",
    "                  \"latitude\",\"longitude\",\"city\",\"zipcode\",\"state\",\"accommodates\",\n",
    "                  \"room_type\",\"bedrooms\",\"bathrooms\",\"beds\",\"price\",\"cleaning_fee\",\n",
    "                  \"security_deposit\",\"minimum_nights\",\"maximum_nights\",\"number_of_reviews\"]\n",
    "\n",
    "rio_listings = pd.read_csv(\"listings.csv\",usecols=target_columns,low_memory=False)\n",
    "\n",
    "# cleaning & preparing\n",
    "stripped_commas = rio_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "rio_listings['price'] = stripped_dollars.astype('float')\n",
    "\n",
    "# separte data into train and test (75%/25%)\n",
    "train_df = rio_listings.iloc[0:int(rio_listings.shape[0]*0.75)]\n",
    "test_df = rio_listings.iloc[int(rio_listings.shape[0]*0.75):]\n",
    "\n",
    "def predict_price(new_listing):\n",
    "    temp_df = rio_listings.copy()\n",
    "    temp_df['distance'] = temp_df['accommodates'].apply(lambda x: np.abs(x - new_listing))\n",
    "    temp_df = temp_df.sort_values('distance')\n",
    "    nearest_neighbor_prices = temp_df.iloc[0:5]['price']\n",
    "    predicted_price = nearest_neighbor_prices.mean()\n",
    "    return(predicted_price)\n",
    "  \n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EwvZvPckPFj"
   },
   "source": [
    "## 2.2 - Error Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltbNGVK-AVVL"
   },
   "source": [
    "\n",
    "\n",
    "We now need a metric that quantifies how good the predictions were on the test set. This class of metrics is called an **error metric**. As the name suggests, an error metric quantifies how inaccurate our predictions were from the actual values. In our case, the error metric tells us how off our predicted price values were from the actual price values for the living spaces in the test dataset.\n",
    "\n",
    "We could start by calculating the difference between each predicted and actual value and then averaging these differences. This is referred to as **mean error** but isn't an effective error metric for most cases. Mean error treats a positive difference differently than a negative difference, but we're really interested in how far off the prediction is in either the positive or negative direction. If the true price was 200 dollars and the model predicted 210 or 190 it's off by 10 dollars either way.\n",
    "\n",
    "We can instead use the **mean absolute error**, where we compute the absolute value of each error before we average all the errors.\n",
    "\n",
    "$\\displaystyle MAE = \\frac{\\left | actual_1 - predicted_1 \\right | + \\left | actual_2 - predicted_2 \\right | + \\\n",
    "\\ldots + \\left | actual_n - predicted_n \\right | }{n}$\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Use **numpy.absolute()** to calculate the mean absolute error between **predicted_price** and **price**.\n",
    "2. Assign the MAE to **mae**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wleEsD9CZau"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1HV9LOukPFk"
   },
   "source": [
    "## 2.3 - Mean Squared Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTt-ZueWA5yY"
   },
   "source": [
    "\n",
    "\n",
    "For many prediction tasks, we want to penalize predicted values that are further away from the actual value much more than those that are closer to the actual value.\n",
    "\n",
    "We can instead take the mean of the squared error values, which is called the **mean squared error** or MSE for short. The MSE makes the gap between the predicted and actual values more clear. A prediction that's off by 100 dollars will have an error (of 10,000) that's 100 times more than a prediction that's off by only 10 dollars (which will have an error of 100).\n",
    "\n",
    "Here's the formula for MSE:\n",
    "\n",
    "$\\displaystyle MSE = \\frac{(actual_1 - predicted_1)^2 + (actual_2 - predicted_2)^2 + \\\n",
    "\\ldots + (actual_n - predicted_n)^2 }{n}$\n",
    "\n",
    "where **n** represents the number of rows in the test set. Let's calculate the MSE value for the predictions we made on the test set.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Calculate the MSE value between the **predicted_price** and **price** columns and assign to **mse**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EoFisr0TCAdT"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0Cp8YWakPFk"
   },
   "source": [
    "## 2.4 -  Training another model\n",
    "\n",
    "The model we trained achieved a mean squared error of around **3383052**. Is this a high or a low mean squared error value? What does this tell us about the quality of the predictions and the model? By itself, the mean squared error value for a single model isn't all that useful.\n",
    "\n",
    "The units of mean squared error in our case is dollars squared (not dollars), which makes it hard to reason about intuitively as well. We can, however, train another model and then compare the mean squared error values to see which model performs better on a relative basis. Recall that a low error metric means that the gap between the predicted list price and actual list price values is low while a high error metric means the gap is high.\n",
    "\n",
    "Let's train another model, this time using the **bathrooms** column, and compare MSE values.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "\n",
    "1. Modify the **predict_price** function below to use the **bathrooms** column instead of the **accommodates** column to make predictions.\n",
    "2. Apply the function to **test_df** and assign the resulting Series object containing the predicted price values to the **predicted_price** column in **test_df**.\n",
    "3. Calculate the squared error between the price and **predicted_price** columns in **test_df** and assign the resulting Series object to the **squared_error** column in **test_df**.\n",
    "4. Calculate the mean of the **squared_error** column in **test_df** and assign to **mse**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaAapmBTkPFl"
   },
   "outputs": [],
   "source": [
    "# separte data into train and test (75%/25%)\n",
    "train_df = rio_listings.iloc[0:int(rio_listings.shape[0]*0.75)]\n",
    "test_df = rio_listings.iloc[int(rio_listings.shape[0]*0.75):]\n",
    "\n",
    "def predict_price(new_listing):\n",
    "    temp_df = train_df.copy()\n",
    "    temp_df['distance'] = temp_df['accommodates'].apply(lambda x: np.abs(x - new_listing))\n",
    "    temp_df = temp_df.sort_values('distance')\n",
    "    nearest_neighbors_prices = temp_df.iloc[0:5]['price']\n",
    "    predicted_price = nearest_neighbors_prices.mean()\n",
    "    return(predicted_price)\n",
    "  \n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smAy-ySjkPFp"
   },
   "source": [
    "## 2.5 - Root Mean Squared Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-wSXNibRL77"
   },
   "source": [
    "\n",
    "\n",
    "While comparing MSE values helps us identify which model performs better on a relative basis, it doesn't help us understand if the performance is good enough in general. This is because the units of the MSE metric are squared (in this case, dollars squared). An MSE value of 2603479 dollars squared doesn't give us an intuitive sense of how far off the model's predictions are systematically off from the true price value in dollars.\n",
    "\n",
    "**Root mean squared error** is an error metric whose units are the base unit (in our case, dollars). RMSE for short, this error metric is calculated by taking the square root of the MSE value:\n",
    "\n",
    "$\\displaystyle RMSE=\\sqrt{MSE}$\n",
    "\n",
    "Since the RMSE value uses the same units as the target column, we can understand how far off in real dollars we can expect the model to perform. For example, if a model achieves an RMSE value of greater than 100, we can expect the predicted price value to be off by 100 dollars on average.\n",
    "\n",
    "Let's calculate the RMSE value of the model we trained using the **bathrooms** column.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "1. Calculate the RMSE value of the model we trained using the **bathrooms** column and assign it to **rmse**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJh970BBRl4I"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8p8M0vRrZgAJ"
   },
   "source": [
    "# 3 - Multivariate K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pTqcY7MZNaz"
   },
   "source": [
    "## 3.1. Recap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOEkNkGDiMKJ"
   },
   "source": [
    "\n",
    "\n",
    "In the last two sections, we explored how to use a simple k-nearest neighbors machine learning model that used just one feature, or attribute, of the listing to predict the rent price. We first relied on the **accommodates** column, which describes the number of people a living space can comfortably accommodate. Then, we switched to the **bathrooms** column and observed an improvement in accuracy. \n",
    "\n",
    "While these were good features to become familiar with the basics of machine learning, it's clear that using just a single feature to compare listings doesn't reflect the reality of the market. An apartment that can accommodate 4 guests in a popular part of **Rio de Janeiro**. will rent for much higher than one that can accommodate 4 guests in a crime ridden area.\n",
    "\n",
    "There are **2 ways** we can tweak the model to try **to improve the accuracy** (decrease the RMSE during validation):\n",
    "\n",
    "- **increase the number of attributes** the model uses to calculate similarity when ranking the closest neighbors\n",
    "- **increase k**, the number of nearby neighbors the model uses when computing the prediction\n",
    "\n",
    "\n",
    "In this section, we'll focus on increasing the number of attributes the model uses. When selecting more attributes to use in the model, we need to watch out for columns that don't work well with the distance equation. This includes columns containing:\n",
    "\n",
    "- **non-numerical values** (e.g. city or state)\n",
    "    - Euclidean distance equation expects numerical values\n",
    "- **missing values**\n",
    "    - distance equation expects a value for each observation and attribute\n",
    "- **non-ordinal values** (e.g. latitude or longitude)\n",
    "    - ranking by Euclidean distance doesn't make sense if all attributes aren't ordinal\n",
    "    \n",
    "In the following code screen, we've read the **listings.csv** dataset from the last section into pandas and brought over the data cleaning changes we made. Let's first look at the first row's values to identify any columns containing non-numerical or non-ordinal values. In the next screen, we'll drop those columns and then look for missing values in each of the remaining columns.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "\n",
    "1. Use the **DataFrame.info()** method to return the number of non-null values in each column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VWSVHnVZNa0"
   },
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "# Brought along the changes we made to the 'rio_listings' Dataframe.\n",
    "target_columns = [\"host_response_rate\", \"host_acceptance_rate\", \"host_listings_count\",\n",
    "                  \"latitude\",\"longitude\",\"city\",\"zipcode\",\"state\",\"accommodates\",\n",
    "                  \"room_type\",\"bedrooms\",\"bathrooms\",\"beds\",\"price\",\"cleaning_fee\",\n",
    "                  \"security_deposit\",\"minimum_nights\",\"maximum_nights\",\"number_of_reviews\"]\n",
    "\n",
    "rio_listings = pd.read_csv(\"listings.csv\",usecols=target_columns,low_memory=False)\n",
    "\n",
    "rio_listings = rio_listings.loc[np.random.permutation(len(rio_listings))]\n",
    "stripped_commas = rio_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "rio_listings['price'] = stripped_dollars.astype('float')\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxp5QS1BZNa3"
   },
   "source": [
    "## 3.2 - Removing features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYpZfziqkXJH"
   },
   "source": [
    "\n",
    "\n",
    "The following columns contain non-numerical values:\n",
    "\n",
    "- **room_type**: e.g. **Private room**\n",
    "- **city**: e.g. **Rio de Janeiro**\n",
    "- **state**: e.g. **RJ**\n",
    "\n",
    "while these columns contain numerical but non-ordinal values:\n",
    "\n",
    "- **latitude**: e.g. **-22.92**\n",
    "- **longitude**: e.g. **-43.23**\n",
    "- **zipcode**: e.g. **20550-012**\n",
    "\n",
    "\n",
    "Geographic values like these aren't ordinal, because a smaller numerical value doesn't directly correspond to a smaller value in a meaningful way. For example, the zip code 20009 isn't smaller or larger than the zip code 75023 and instead both are unique, identifier values. Latitude and longitude value pairs describe a point on a geographic coordinate system and different equations are used in those cases (e.g. [haversine](https://en.wikipedia.org/wiki/Haversine_formula)).\n",
    "\n",
    "While we could convert the **host_response_rate** and **host_acceptance_rate** columns to be numerical (right now they're object data types and contain the **%** sign), these columns describe the host and not the living space itself. Since a host could have many living spaces and we don't have enough information to uniquely group living spaces to the hosts themselves, let's avoid using any columns that don't directly describe the living space or the listing itself:\n",
    "\n",
    "- **host_response_rate**\n",
    "- **host_acceptance_rate**\n",
    "- **host_listings_count**\n",
    "\n",
    "Let's remove these 9 columns from the Dataframe\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Remove the 9 columns we discussed above from **rio_listings**:\n",
    "    - 3 containing non-numerical values\n",
    "    - 3 containing numerical but non-ordinal values\n",
    "    - 3 describing the host instead of the living space itself\n",
    "2. Verify the number of null values of each remain columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0d-Goy5XlM0P"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8V1be1QLZNa4"
   },
   "source": [
    "## 3.3 - Handling missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ix-X5iZnsxzF"
   },
   "source": [
    "\n",
    "Of the remaining columns, 3 columns have a few missing values (less than 1% of the total number of rows):\n",
    "\n",
    "- **bedrooms**\n",
    "- **bathrooms**\n",
    "- **beds**\n",
    "\n",
    "Since the number of rows containing missing values for one of these 3 columns is low, we can select and remove those rows without losing much information. There are also 2 columns have a large number of missing values:\n",
    "\n",
    "- **cleaning_fee** - 37.81% of the rows\n",
    "- **security_deposit** - 50.22% of the rows\n",
    "\n",
    "and we can't handle these easily. We can't just remove the rows containing missing values for these 2 columns because we'd miss out on the majority of the observations in the dataset. Instead, let's remove these 2 columns entirely from consideration.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Drop the **cleaning_fee** and **security_deposit** columns from **rio_listings**.\n",
    "2. Then, remove all rows that contain a missing value for the **bedrooms**, **bathrooms**, or **beds** column from **rio_listings**.\n",
    "    - You can accomplish this by using the [Dataframe method dropna()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) and setting the **axis** parameter to **0**.\n",
    "    - Since only the **bedrooms**, **bathrooms** and **beds** columns contain any missing values, rows containing missing values in these columns will be removed.\n",
    "3. Display the null value counts for the updated **rio_listings** Dataframe to confirm that there are no missing values left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZS1f7LmweoC"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyUZLEQeZNa5"
   },
   "source": [
    "## 3.4 -  Normalize columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCRHHpK6w2Vo"
   },
   "source": [
    "\n",
    "\n",
    "Here's how the **rio_listings** Dataframe looks like after all the changes we made:\n",
    "\n",
    "|  |accommodates | bathrooms | bedrooms | beds | price | minimum_nights | maximum_nights | number_of_reviews |    \n",
    "|--------------|-----------|----------|------|-------|----------------|----------------|-------------------|----|\n",
    "| 3629         | 8         | 1.0      | 1.0  | 1.0   | 200.0          | 2              | 60                | 0  |\n",
    "| 15971        | 6         | 2.0      | 3.0  | 4.0   | 901.0          | 10             | 27                | 0  |\n",
    "| 34100        | 2         | 1.0      | 1.0  | 1.0   | 229.0          | 2              | 50                | 24 |\n",
    "| 35013        | 2         | 1.0      | 1.0  | 2.0   | 200.0          | 3              | 1125              | 0  |\n",
    "| 35992        | 2         | 2.0      | 1.0  | 1.0   | 75.0           | 2              | 30                | 2  |\n",
    "\n",
    "You may have noticed that while the **accommodates**, **bedrooms**, **bathrooms**, **beds**, and **minimum_nights** columns hover between 0 and 10 (at least in the first few rows), the values in the **maximum_nights** and **number_of_reviews** columns span much larger ranges. For example, the **maximum_nights** column has values as low as 27 and high as 1125, in the first few rows itself. If we use these 2 columns as part of a k-nearest neighbors model, these attributes could end up having an outsized effect on the distance calculations because of the largeness of the values.\n",
    "\n",
    "For example, 2 living spaces could be identical across every attribute but be vastly different just on the **maximum_nights** column. If one listing had a **maximum_nights** value of 1125 and the other a **maximum_nights** value of 27, because of the way Euclidean distance is calculated, these listings would be considered very far apart because of the outsized effect the largeness of the values had on the overall Euclidean distance. To prevent any single column from having too much of an impact on the distance, we can **normalize** all of the columns to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Normalizing the values in each columns to the [standard normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution) (mean of 0, standard deviation of 1) preserves the distribution of the values in each column while aligning the scales. To normalize the values in a column to the standard normal distribution, you need to:\n",
    "\n",
    "- from each value, subtract the mean of the column\n",
    "- divide each value by the standard deviation of the column\n",
    "\n",
    "Here's the mathematical formula describing the transformation that needs to be applied for all values in a column:\n",
    "\n",
    "$\\displaystyle z= \\frac{x − \\mu}{\\sigma}$\n",
    "\n",
    "where x is a value in a specific column, $\\mu$ is the mean of all the values in the column, and $\\sigma$ is the standard deviation of all the values in the column. Here's what the corresponding code, using pandas, looks like:\n",
    "\n",
    "```python\n",
    "# Subtract each value in the column by the mean.\n",
    "first_transform = rio_listings['maximum_nights'] - rio_listings['maximum_nights'].mean()\n",
    "# Divide each value in the column by the standard deviation.\n",
    "normalized_col = first_transform / rio_listings['maximum_nights'].std()\n",
    "```\n",
    "\n",
    "To apply this transformation across all of the columns in a Dataframe, you can use the corresponding Dataframe methods mean() and std():\n",
    "\n",
    "```python\n",
    "normalized_listings = (rio_listings - rio_listings.mean()) / (rio_listings.std())\n",
    "```\n",
    "\n",
    "These methods were written with mass column transformation in mind and when you call **mean()** or **std()**, the appropriate column means and column standard deviations are used for each value in the Dataframe. Let's now normalize all of the feature columns in **rio_listings**.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Normalize all of the feature columns in **rio_listings** and assign the new Dataframe containing just the normalized feature columns to **normalized_listings**.\n",
    "2. Add the price column from **rio_listings** to **normalized_listings**.\n",
    "3. Display the first 3 rows in **normalized_listings**.\n",
    "4. Use the function **describe** to see min and max values in **normalized_listings**.\n",
    "5. Don't forget to eliminate outliers under **normalized_listings**. (note: this can be a little bit more difficult than previous exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7sQr0K2z299"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UniSmnEhZNa7"
   },
   "source": [
    "## 3.5 - Euclidean distance for multivariate case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUZeFvA11SH"
   },
   "source": [
    "\n",
    "\n",
    "In the last section, we trained 2 univariate k-nearest neighbors models. The first one used the **accommodates** attribute while the second one used the **bathrooms** attribute. Let's now train a model that uses **both** attributes when determining how similar 2 living spaces are. Let's refer to the Euclidean distance equation again to see what the distance calculation using 2 attributes would look like:\n",
    "\n",
    "$\\displaystyle d = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + \\ldots + (q_n - p_n)^2}$\n",
    "\n",
    "Since we're using 2 attributes, the distance calculation would look like:\n",
    "\n",
    "$\\displaystyle d = \\sqrt{(accommodates_1 - accomodates_2)^2 + (bathrooms_1 - bathrooms_2)^2}$\n",
    "\n",
    "\n",
    "To find the distance between 2 living spaces, we need to calculate the squared difference between both **accommodates** values, the squared difference between both **bathrooms** values, add them together, and then take the square root of the resulting sum. Here's what the Euclidean distance between the first 2 rows in **normalized_listings** looks like:\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=15uoTMT1rzRLx9T8kIbsOWw7HaTmdBP0o\">\n",
    "\n",
    "\n",
    "So far, we've been calculating Euclidean distance ourselves by writing the logic for the equation ourselves. We can instead use the [distance.euclidean()](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.euclidean.html) function from **scipy.spatial**, which takes in 2 vectors as the parameters and calculates the Euclidean distance between them. The **euclidean()** function expects:\n",
    "\n",
    "- both of the vectors to be represented using a **list-like** object (Python list, NumPy array, or pandas Series)\n",
    "- both of the vectors must be 1-dimensional and have the same number of elements\n",
    "\n",
    "Here's a simple example:\n",
    "\n",
    "```python\n",
    "from scipy.spatial import distance\n",
    "first_listing = [-0.596544, -0.439151]\n",
    "second_listing = [-0.596544, 0.412923]\n",
    "dist = distance.euclidean(first_listing, second_listing)\n",
    "```\n",
    "\n",
    "Let's use the **euclidean()** function to calculate the Euclidean distance between 2 rows in our dataset to practice.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Calculate the Euclidean distance using only the **accommodates** and **bathrooms** features between the first row and fifth row in **normalized_listings** using the **distance.euclidean()** function.\n",
    "2. Assign the distance value to **first_fifth_distance** and display using the **print** function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcczgNiV3Dwf"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MARVt0PcZNa8"
   },
   "source": [
    "## 3.6 -  Introduction to scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OkTas2b37iJ"
   },
   "source": [
    "\n",
    "\n",
    "So far, we've been writing functions from scratch to train the **k-nearest neighbor models**. While this is helpful deliberate practice to understand how the mechanics work, you can be more productive and iterate quicker by using a library that handles most of the implementation. In this subsection, we'll learn about the [scikit-learn library](http://scikit-learn.org/), which is the most popular machine learning in Python. Scikit-learn contains functions for all of the major machine learning algorithms and a simple, unified workflow. Both of these properties allow data scientists to be incredibly productive when training and testing different models on a new dataset.\n",
    "\n",
    "The scikit-learn workflow consists of 4 main steps:\n",
    "\n",
    "- instantiate the specific machine learning model you want to use\n",
    "- fit the model to the training data\n",
    "- use the model to make predictions\n",
    "- evaluate the accuracy of the predictions\n",
    "\n",
    "\n",
    "We'll focus on the first 3 steps in this section. Each model in scikit-learn is implemented as a [separate class](http://scikit-learn.org/dev/modules/classes.html) and the first step is to identify the class we want to create an instance of. In our case, we want to use the [KNeighborsRegressor class](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor).\n",
    "Any model that helps us predict numerical values, like listing price in our case, is known as a **regression** model. The other main class of machine learning models is called classification, where we're trying to predict a label from a fixed set of labels (e.g. blood type or gender). The word **regressor** from the class name **KNeighborsRegressor** refers to the regression model class that we just discussed.\n",
    "\n",
    "Scikit-learn uses a similar object-oriented style to Matplotlib and you need to instantiate an empty model first by calling the constructor:\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "```\n",
    "\n",
    "If you refer to the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor), you'll notice that by default:\n",
    "\n",
    "- **n_neighbors:**  the number of neighbors, is set to **5**\n",
    "- **algorithm:** for computing nearest neighbors, is set to **auto**\n",
    "- **p:** set to **2**, corresponding to Euclidean distance\n",
    "\n",
    "Let's set the **algorithm** parameter to **brute** and leave the **n_neighbors** value as **5**, which matches the implementation we wrote in the last mission. If we leave the **algorithm** parameter set to the default value of **auto**, scikit-learn will try to use tree-based optimizations to improve performance (which are outside of the scope of this lesson):\n",
    "\n",
    "```python\n",
    "knn = KNeighborsRegressor(algorithm='brute')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HjiPj2x9ZNa9"
   },
   "source": [
    "## 3.7 - Fitting a model and making predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWNIHM_T_jPl"
   },
   "source": [
    "\n",
    "\n",
    "Now, we can fit the model to the data using the [fit method](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor.fit). For all models, the **fit** method takes in 2 required parameters:\n",
    "\n",
    "- matrix-like object, containing the feature columns we want to use from the training set.\n",
    "- list-like object, containing correct target values.\n",
    "\n",
    "Matrix-like object means that the method is flexible in the input and either a Dataframe or a NumPy 2D array of values is accepted. This means you can select the columns you want to use from the Dataframe and use that as the first parameter to the **fit** method.\n",
    "\n",
    "If you recall from earlier in the mission, all of the following are acceptable list-like objects:\n",
    "\n",
    "- NumPy array\n",
    "- Python list\n",
    "- pandas Series object (e.g. when selecting a column)\n",
    "\n",
    "You can select the target column from the Dataframe and use that as the second parameter to the **fit** method:\n",
    "\n",
    "\n",
    "```python\n",
    "# Split full dataset into train and test sets.\n",
    "train_df = normalized_listings.iloc[0:int(normalized_listings.shape[0]*0.75)]\n",
    "test_df = normalized_listings.iloc[int(normalized_listings.shape[0]*0.75):]\n",
    "# Matrix-like object, containing just the 2 columns of interest from training set.\n",
    "train_features = train_df[['accommodates', 'bathrooms']]\n",
    "# List-like object, containing just the target column, `price`.\n",
    "train_target = normalized_listings['price']\n",
    "# Pass everything into the fit method.\n",
    "knn.fit(train_features, train_target)\n",
    "```\n",
    "\n",
    "\n",
    "When the **fit** method is called, scikit-learn stores the training data we specified within the KNearestNeighbors instance (**knn**). If you try passing in data containing missing values or non-numerical values into the **fit** method, scikit-learn will return an error. Scikit-learn contains many such features that help prevent us from making common mistakes.\n",
    "\n",
    "Now that we specified the training data we want used to make predictions, we can use the [predict method](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor.predict) to make predictions on the test set. The **predict** method has only one required parameter:\n",
    "\n",
    "- matrix-like object, containing the feature columns from the dataset we want to make predictions on\n",
    "\n",
    "The number of feature columns you use during both training and testing need to match or scikit-learn will return an error:\n",
    "\n",
    "```python\n",
    "predictions = knn.predict(test_df[['accommodates', 'bathrooms']])\n",
    "```\n",
    "\n",
    "The **predict()** method returns a NumPy array containing the predicted **price** values for the test set. You now have everything you need to practice the entire scikit-learn workflow.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "1. Create an instance of the [KNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) class with the following parameters:\n",
    "    - **n_neighbors**: 5\n",
    "    - **algorithm**: brute\n",
    "2. Use the **fit** method to specify the data we want the k-nearest neighbor model to use. Use the following parameters:\n",
    "    - training data, feature columns: just the **accommodates** and **bathrooms** columns, in that order, from **train_df**.\n",
    "    - training data, target column: the **price** column from **train_df**.\n",
    "3. Call the **predict** method to make predictions on:\n",
    "    - the **accommodates** and **bathrooms** columns from **test_df**\n",
    "    - assign the resulting NumPy array of predicted price values to **predictions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXSu8QoGZNa9"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Split full dataset into train and test sets.\n",
    "train_df = normalized_listings.iloc[0:int(normalized_listings.shape[0]*0.75)]\n",
    "test_df = normalized_listings.iloc[int(normalized_listings.shape[0]*0.75):]\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLKowdxzZNbA"
   },
   "source": [
    "## 3.8 - Calculating MSE using Scikit-Learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYFgbObLC4K8"
   },
   "source": [
    "\n",
    "\n",
    "Earlier in this mission, we calculated the MSE and RMSE values using the pandas arithmetic operators to compare each predicted value with the actual value from the **price** column of our test set. Alternatively, we can instead use the [sklearn.metrics.mean_squared_error function()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error). Once you become familiar with the different machine learning concepts, unifying your workflow using scikit-learn helps save you a lot of time and avoid mistakes.\n",
    "\n",
    "The **mean_squared_error()** function takes in 2 inputs:\n",
    "\n",
    "- list-like object, representing the true values\n",
    "- list-like object, representing the predicted values using the model\n",
    "\n",
    "For this function, we won't show any sample code and will leave it to you to understand the function from the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) itself to calculate the MSE and RMSE values for the predictions we just made.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Use the **mean_squared_error** function to calculate the MSE value for the predictions we made in the previous exercise.\n",
    "2. Assign the MSE value to **two_features_mse**.\n",
    "3. Calculate the RMSE value by taking the square root of the MSE value and assign to **two_features_rmse**.\n",
    "4. Display both of these error scores using the **print** function.\n",
    "5. Using more features and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0K359qjkZNbB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# features\n",
    "train_columns = ['accommodates', 'bathrooms']\n",
    "\n",
    "# instantiate a knn object\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='brute', metric='euclidean')\n",
    "\n",
    "# train the model\n",
    "knn.fit(train_df[train_columns], train_df['price'])\n",
    "\n",
    "# predict\n",
    "predictions = knn.predict(test_df[train_columns])\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZF3VaXoUkm2"
   },
   "source": [
    "# 4 - Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqHvG-2kUl8K"
   },
   "source": [
    "## 4.1 Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYK97otjWEhs"
   },
   "source": [
    "In the last section, we focused on increasing the number of attributes the model uses. We saw how, in general, adding more attributes generally lowered the error of the model. This is because the model is able to do a better job identifying the living spaces from the training set that are the most similar to the ones from the test set. However, we also observed how using all of the available features didn't actually improve the model's accuracy automatically and that some of the features were probably not relevant for similarity ranking. We learned that selecting relevant features was the right lever when improving a model's accuracy, not just increasing the features used in the absolute.\n",
    "\n",
    "In this section, we'll focus on the impact of increasing **k**, the number of nearby neighbors the model uses to make predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9ElhdYCYdf5"
   },
   "outputs": [],
   "source": [
    "# Split full dataset into train and test sets.\n",
    "train_df = normalized_listings.iloc[0:int(normalized_listings.shape[0]*0.75)]\n",
    "test_df = normalized_listings.iloc[int(normalized_listings.shape[0]*0.75):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EXp7NSmsUya"
   },
   "source": [
    "## 4.2 - Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfDZI_yzsaSi"
   },
   "source": [
    "When we vary the features that are used in the model, we're affecting the data that the model uses. On the other hand, varying the k value affects the behavior of the model independently of the actual data that's used when making predictions. In other words, we're impacting how the model performs without trying to change the data that's used.\n",
    "\n",
    "Values that affect the behavior and performance of a model that are unrelated to the data that's used are referred to as **hyperparameters**. The process of finding the optimal hyperparameter value is known as **hyperparameter optimization**. A simple but common [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) technique is known as [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search), which involves:\n",
    "\n",
    "- selecting a subset of the possible hyperparameter values,\n",
    "- training a model using each of these hyperparameter values,\n",
    "- evaluating each model's performance,\n",
    "- selecting the hyperparameter value that resulted in the lowest error value.\n",
    "\n",
    "Grid search essentially boils down to evaluating the model performance at different k values and selecting the k value that resulted in the lowest error. While grid search can take a long time when working with large datasets, the data we're working with in this section is small and this process is relatively quick.\n",
    "\n",
    "Let's confirm that grid search will work quickly for the dataset we're working with by first observing how the model performance changes as we increase the k value from **1** to **5**. If you recall, we set **5** as the **k** value for the last 2 sections. Let's use the features from the last mission:\n",
    "\n",
    "- **accommodates**\n",
    "- **bedrooms**\n",
    "- **bathrooms**\n",
    "- **number_of_reviews**\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "1. Create a list containing the integer values **1**, **2**, **3**, $\\ldots$, **20**, in that order, and assign to **hyper_params**.\n",
    "2. Create an empty list and assign to **mse_values**.\n",
    "3. Use a **for loop** to iterate over **hyper_params** and in each iteration:\n",
    "    - Instantiate a **KNeighborsRegressor** object with the following parameters:\n",
    "        - **n_neighbors**: the current value for the iterator variable,\n",
    "        - **algorithm**: brute\n",
    "    - Fit the instantiated k-nearest neighbors model to the following columns from **train_df**:\n",
    "        - **accommodates**\n",
    "        - **bedrooms**\n",
    "        - **bathrooms**\n",
    "        - **number_of_reviews**\n",
    "    - Use the trained model to make predictions on the same columns from **test_df** and assign to **predictions**.\n",
    "    - Use the **mean_squared_error** function to calculate the MSE value between **predictions** and the **price** column from **test_df**.\n",
    "    - Append the MSE value to **mse_values**.\n",
    "4. Display **mse_values** using the **print()** function.\n",
    "5. Calculate the **rmse** from **mse_values**. Use **print()** function to display **rmse**. \n",
    "6. Use the seaborn to generate a scatter plot with:\n",
    "    - **hyper_params** on the x-axis,\n",
    "    - **mse_values** on the y-axis.\n",
    "7. Use plt.show() to display the scatter plot.\n",
    "8. Try to find a combination of features which reach the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEgoeY0FtcJF"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpvNmqHt46sT"
   },
   "source": [
    "# 5 - Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fn6KIEUx7fyt"
   },
   "source": [
    "## 5.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YtCPPoq7smj"
   },
   "source": [
    "\n",
    "In an earlier sections, we learned about train/test validation, a simple technique for testing a machine learning model's accuracy on new data that the model wasn't trained on. In this section, we'll focus on more robust techniques.\n",
    "\n",
    "To start, we'll focus on the **holdout validation** technique, which involves:\n",
    "\n",
    "- splitting the full dataset into 2 partitions:\n",
    "    - a training set\n",
    "    - a test set\n",
    "- training the model on the training set,\n",
    "- using the trained model to predict values on the test set,\n",
    "- computing an error metric to understand the model's effectiveness,\n",
    "- switch the training and test sets and repeat,\n",
    "- average the errors.\n",
    "\n",
    "In holdout validation, we usually use a 50/50 split instead of the 75/25 split from train/test validation. This way, we remove number of observations as a potential source of variation in our model performance.\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Nwq2puuGhziyQ82eukrPctHQ8UGJ93Vt\">\n",
    "\n",
    "Let's start by splitting the data set into 2 nearly equivalent halves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNt7jfUd9yNl"
   },
   "outputs": [],
   "source": [
    "# Split full dataset into two splits dataset\n",
    "split_one_df = normalized_listings.iloc[0:int(normalized_listings.shape[0]*0.5)]\n",
    "split_two_df = normalized_listings.iloc[int(normalized_listings.shape[0]*0.5):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVkrBZx1-9hc"
   },
   "source": [
    "## 5.2 - Holdout Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p29KJ4TPDw4s"
   },
   "source": [
    "\n",
    "\n",
    "Now that we've split our data set into 2 dataframes, let's:\n",
    "\n",
    "- train a k-nearest neighbors model on the first half,\n",
    "- test this model on the second half,\n",
    "- train a k-nearest neighbors model on the second half,\n",
    "- test this model on the first half.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n",
    "\n",
    "- Train a k-nearest neighbors model using the default algorithm (**auto**) and the default number of neighbors (**5**) that:\n",
    "    - Uses the **accommodates** column from **train_one** for training and\n",
    "    - Tests it on  **test_one**.\n",
    "- Assign the resulting RMSE value to **iteration_one_rmse**.\n",
    "- Train a k-nearest neighbors model using the default algorithm (**auto**) and the default number of neighbors (**5**) that:\n",
    "    - Uses the **accommodates** column from **train_two** for training and\n",
    "    - Tests it on  **test_two**.\n",
    "- Assign the resulting RMSE value to **iteration_two_rmse**.\n",
    "- Use **numpy.mean()** to calculate the average of the 2 RMSE values and assign to **avg_rmse**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHmkRNHtFGj1"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2gRYcU16k0S"
   },
   "source": [
    "## 5.3 - K-Fold Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNkucLj8G4Vf"
   },
   "source": [
    "\n",
    "**Holdout validation** is actually a specific example of a larger class of validation techniques called **k-fold cross-validation**. While holdout validation is better than train/test validation because the model isn't repeatedly biased towards a specific subset of the data, both models that are trained only use half the available data. K-fold cross validation, on the other hand, takes advantage of a larger proportion of the data during training while still rotating through different subsets of the data to avoid the issues of train/test validation.\n",
    "\n",
    "Here's the algorithm from k-fold cross validation:\n",
    "\n",
    "- splitting the full dataset into **k** equal length partitions,\n",
    "    - selecting **k-1** partitions as the training set and\n",
    "    - selecting the remaining partition as the test set\n",
    "- training the model on the training set,\n",
    "- using the trained model to predict labels on the test fold,\n",
    "- computing the test fold's error metric,\n",
    "- repeating all of the above steps **k-1** times, until each partition has been used as the test set for an iteration,\n",
    "- calculating the mean of the **k** error values.\n",
    "\n",
    "Holdout validation is essentially a version of k-fold cross validation when **k** is equal to **2**. Generally, **5** or **10** folds is used for k-fold cross-validation. Here's a diagram describing each iteration of 5-fold cross validation:\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1i9sScUbJqDLeCtd3InrBbaeHXI7vkAyQ\">\n",
    "\n",
    "\n",
    "As you increase the number the folds, the number of observations in each fold decreases and the variance of the fold-by-fold errors increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-sD-kyI6k0a"
   },
   "source": [
    "## 5.4 - Performing K-Fold Cross Validation Using Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5oklMw3IXEr"
   },
   "source": [
    "In machine learning, we're interested in building a good model and accurately understand how well it will perform. To build a better k-nearest neighbors model, we can change the features it uses or tweak the number of neighbors (a hyperparameter). To accurately understand a model's performance, we can perform k-fold cross validation and select the proper number of folds. We've learned how scikit-learn makes it easy for us to quickly experiment with these different knobs when it comes to building a better model. Let's now dive into how we can use scikit-learn to handle cross-validation as well.\n",
    "\n",
    "\n",
    "First, we instantiate an instance of the [KFold class](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) from  **sklearn.model_selection**:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_folds, shuffle=False, random_state=None)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- **n_folds** is the number of folds you want to use,\n",
    "- **shuffle** is used to toggle shuffling of the ordering of the observations in the dataset,\n",
    "- **random_state** is used to specify the random seed value if **shuffle** is set to **True**.\n",
    "\n",
    "\n",
    "You'll notice here that no parameters depend on the data set at all. This is because the KFold class returns an iterator object which we use in conjunction with the [cross_val_score()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function, also from **sklearn.model_selection**. Together, these 2 functions allow us to compactly train and test using k-fold cross validation:\n",
    "\n",
    "Here are the relevant parameters for the **cross_val_score** function:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(estimator, X, Y, scoring=None, cv=None)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- **estimator** is a sklearn model that implements the **fit** method (e.g. instance of **KNeighborsRegressor**),\n",
    "-  **X** is the list or 2D array containing the features you want to train on,\n",
    "- **Y** is a list containing the values you want to predict (target column),\n",
    "- **scoring** is a string describing the scoring criteria (list of accepted values [here](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)).\n",
    "    - Some evaluation metrics (like mean squared error) are naturally descending scores (the smallest score is best) and as such are reported as negative by the  **cross_val_score()** function. This is important to note, because some scores will be reported as negative that by definition can never be negative.\n",
    "- **cv** describes the number of folds. Here are some examples of accepted values:\n",
    "    - an instance of the **KFold** class,\n",
    "    - an integer representing the number of folds.\n",
    "    \n",
    "Depending on the scoring criteria you specify, either a single total value is returned one value for each fold. Here's the general workflow for performing k-fold cross-validation using the classes we just described:\n",
    "\n",
    "- instantiate the scikit-learn model class you want to fit,\n",
    "- instantiate the **KFold** class and using the parameters to specify the k-fold cross-validation attributes you want,\n",
    "- use the **cross_val_score()** function to return the scoring metric you're interested in.\n",
    "\n",
    "**Guided Exercise**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0Wn0_kXKM6t"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# kfold instance\n",
    "kf = KFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "# knn model\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# cross validation (knn,x,y,scoring,kfold)\n",
    "mses = cross_val_score(model, \n",
    "                       normalized_listings[[\"accommodates\"]], \n",
    "                       normalized_listings[\"price\"], \n",
    "                       scoring=\"neg_mean_squared_error\", \n",
    "                       cv=kf)\n",
    "\n",
    "# root mean squared error\n",
    "rmses = np.sqrt(np.absolute(mses))\n",
    "\n",
    "# average error\n",
    "avg_rmse = np.mean(rmses)\n",
    "\n",
    "print(rmses)\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_Nk-qN_6k0g"
   },
   "source": [
    "## 5.5 - Exploring Different K Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FeuHjwLK9D4"
   },
   "source": [
    "\n",
    "\n",
    "Choosing the right **k** value when performing k-fold cross validation is **more of an art and less of a science**. As we discussed earlier in the lesson, a **k** value of **2** is really just holdout validation. On the other end, setting **k** equal to **n** (the number of observations in the data set) is known as **leave-one-out cross validation**, or **LOOCV** for short. Through lots of trial and error, data scientists have converged on **10** as the standard **k** value.\n",
    "\n",
    "In the following code block, we display the results of varying **k** from **3 to 23**. For each **k** value, we calculate and display the average RMSE value across all of the folds and the standard deviation of the RMSE values. \n",
    "\n",
    "**Guided Exercise**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CT6KFKzLL2V4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "\n",
    "num_folds = [3, 5, 7, 9, 10, 11, 13, 15, 17, 19, 21, 23]\n",
    "\n",
    "for fold in num_folds:\n",
    "    kf = KFold(fold, shuffle=True, random_state=1)\n",
    "    model = KNeighborsRegressor()\n",
    "    mses = cross_val_score(model, normalized_listings[[\"accommodates\"]],\n",
    "                           normalized_listings[\"price\"], scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "    rmses = np.sqrt(np.absolute(mses))\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    std_rmse = np.std(rmses)\n",
    "    print(str(fold), \"folds: \", \"avg RMSE: \", str(avg_rmse), \"std RMSE: \", str(std_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOGdrk0tNC4i"
   },
   "source": [
    "## 5.6 - Bias-Variance Tradeoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnra64RlNN22"
   },
   "source": [
    "\n",
    "\n",
    "So far, we've been working under the assumption that a lower RMSE always means that a model is more accurate. This isn't the complete picture, unfortunately. \n",
    "A model has two sources of error:\n",
    "\n",
    "- **bias**\n",
    "- **variance**.\n",
    "\n",
    "**Bias** describes error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that will result in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides just its weight.\n",
    "\n",
    "**Variance** describes error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, we will have low bias but high variance. In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff.\n",
    "\n",
    "The standard deviation of the RMSE values can be a proxy for a model's **variance** while the average RMSE is a proxy for a model's **bias**. Bias and variance are the 2 observable sources of error in a model that we can indirectly control.\n",
    "\n",
    "<img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1HHIrhAPERrQlzslS1dn3PTy78MaAERhu\">\n",
    "\n",
    "\n",
    "While k-nearest negihbors can make predictions, it isn't a mathematical model. A mathematical model is usually an equation that can exist without the original data, which isn't true with k-nearest neighbors. In the next classes, we'll learn about a mathematical model called linear regression. We'll explore the bias-variance tradeoff in greater depth in these next 2 courses because of its importance when working with mathematical models in particular.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gaePLCMj_iD0",
    "23Q2awTs_iD4",
    "jVJxA-wk_iD5",
    "mcJp4JUT_iD9",
    "OpXKHwKWkPFe",
    "6EwvZvPckPFj",
    "-1HV9LOukPFk",
    "g0Cp8YWakPFk",
    "smAy-ySjkPFp",
    "9pTqcY7MZNaz",
    "vxp5QS1BZNa3",
    "8V1be1QLZNa4",
    "wyUZLEQeZNa5",
    "UniSmnEhZNa7",
    "MARVt0PcZNa8",
    "HjiPj2x9ZNa9",
    "oLKowdxzZNbA",
    "pqHvG-2kUl8K",
    "_EXp7NSmsUya",
    "fn6KIEUx7fyt",
    "DVkrBZx1-9hc",
    "W2gRYcU16k0S",
    "X-sD-kyI6k0a",
    "5_Nk-qN_6k0g",
    "wOGdrk0tNC4i"
   ],
   "name": "lesson06_k-nn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

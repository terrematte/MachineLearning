{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle Workflow - Titanic: Machine Learning from Disaster with RandomizedSearch, KNN, XGBoost, AdaBoostClassifier, CatBoost and VotingEnsemble.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iapz_gk4bL4Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - Introducing data science workflows\n"
      ]
    },
    {
      "metadata": {
        "id": "MG9jSDw-wVhY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** From [https://github.com/ivanovitchm/EEC1509_MachineLearning](https://github.com/ivanovitchm/EEC1509_MachineLearning/tree/master/Lesson%20%2311%20-%20Kaggle%20Fundamentals).***\n",
        "\n",
        "In this guided project, we're going to put together all that we've learned in this course and create a data science workflow.\n",
        "\n",
        "By defining a workflow for yourself, you can give yourself a framework with which to make iterating on ideas quicker and easier, allowing yourself to work more efficiently.\n",
        "\n",
        "In this mission, we're going to explore a workflow to make competing in the Kaggle Titanic competition easier, using a pipeline of functions to reduce the number of dimensions you need to focus on.\n",
        "\n",
        "To get started, we'll read in the original **train.csv** and **test.csv** files from Kaggle.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BksilRmPu3f3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!mkdir data predictions\n",
        "#!mv holdout_modified.csv train_modified.csv train.csv test.csv data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWquy7FRbL4R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"./data/train.csv\")\n",
        "holdout = pd.read_csv(\"./data/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BPE04u4RbL4V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P92sW6bzbL4a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "holdout.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpDHDy1PbL4e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "survived = train[\"Survived\"]\n",
        "train = train.drop(\"Survived\",axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXGN8riJbL4h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "holdout.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "sBcr5bYhbL4j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IKIcnJyhbL4n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## concatenate all data to guarantee that dataset have the same columns\n",
        "all_data = pd.concat([train,holdout],axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aGbcWYNjbL4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPZr6A-JbL4s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2 - Exploring the Data\n"
      ]
    },
    {
      "metadata": {
        "id": "TwwXZJhMwsER",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In the first three missions of this course, we have done a variety of activities, mostly in isolation: **Exploring the data**, **creating features**, **selecting features**, **selecting and tuning different models**.\n",
        "\n",
        "The Kaggle workflow we are going to build will combine all of these into a process.\n",
        "\n",
        "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1swb6PxXUJuDvv83ylqh9eUh992lXTu47\">\n",
        "\n",
        "- **Data exploration**, to find patterns in the data\n",
        "- **Feature engineering**, to create new features from those patterns or through pure experimentation\n",
        "- **Feature selection**, to select the best subset of our current set of features\n",
        "- **Model selection/tuning**, training a number of models with different hyperparameters to find the best performer.\n",
        "\n",
        "We can continue to repeat this cycle as we work to optimize our predictions. At the end of any cycle we wish, we can also use our model to make predictions on the holdout set and then **Submit to Kaggle** to get a leaderboard score.\n",
        "\n",
        "While the first two steps of our workflow are relatively freeform, later in this project we'll create some functions that will help automate the complexity of the latter two steps so we can move faster.\n",
        "\n",
        "For now, let's practice the first stage, exploring the data. We're going to examine the two columns that contain information about the family members each passenger had onboard: **SibSp** and **Parch**."
      ]
    },
    {
      "metadata": {
        "id": "RqMGzwrQbL4t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 - Preprocesing the Data"
      ]
    },
    {
      "metadata": {
        "id": "VddvfkrFbL4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_ticket(df):\n",
        "    # see https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n",
        "    Ticket = []\n",
        "    for i in list(df.Ticket):\n",
        "        if not i.isdigit():\n",
        "            #Take prefix\n",
        "            Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) \n",
        "        else:\n",
        "            Ticket.append(\"X\")\n",
        "    df[\"Ticket\"] = Ticket\n",
        "    return df\n",
        "\n",
        "def process_missing(df):\n",
        "    \"\"\"Handle various missing values from the data set\n",
        "\n",
        "    Usage\n",
        "    ------\n",
        "\n",
        "    holdout = process_missing(holdout)\n",
        "    \"\"\"\n",
        "    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n",
        "    df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")\n",
        "    return df\n",
        "\n",
        "def process_age(df):\n",
        "    \"\"\"Process the Age column into pre-defined 'bins' \n",
        "\n",
        "    Usage\n",
        "    ------\n",
        "\n",
        "    train = process_age(train)\n",
        "    \"\"\"\n",
        "    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n",
        "    cut_points = [-1,0,5,12,18,35,60,100]\n",
        "    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n",
        "    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n",
        "    \n",
        "    #df = df.drop(\"Age\",axis=1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def process_fare(df):\n",
        "    \"\"\"Process the Fare column into pre-defined 'bins' \n",
        "\n",
        "    Usage\n",
        "    ------\n",
        "\n",
        "    train = process_fare(train)\n",
        "    \"\"\"\n",
        "    cut_points = [-1,12,50,100,1000]\n",
        "    label_names = [\"0-12\",\"12-50\",\"50-100\",\"100+\"]\n",
        "    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"],cut_points,labels=label_names)\n",
        "    \n",
        "    df = df.drop(\"Fare\",axis=1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def process_cabin(df):\n",
        "    \"\"\"Process the Cabin column into pre-defined 'bins' \n",
        "\n",
        "    Usage\n",
        "    ------\n",
        "\n",
        "    train process_cabin(train)\n",
        "    \"\"\"\n",
        "    df[\"Cabin_type\"] = df[\"Cabin\"].str[0]\n",
        "    df[\"Cabin_type\"] = df[\"Cabin_type\"].fillna(\"Unknown\")\n",
        "    df = df.drop('Cabin',axis=1)\n",
        "    return df\n",
        "\n",
        "def process_titles(df):\n",
        "    \"\"\"Extract and categorize the title from the name column \n",
        "\n",
        "    Usage\n",
        "    ------\n",
        "\n",
        "    train = process_titles(train)\n",
        "    \"\"\"\n",
        "    titles = {\n",
        "        \"Mr\" :         \"Mr\",\n",
        "        \"Mme\":         \"Mrs\",\n",
        "        \"Ms\":          \"Mrs\",\n",
        "        \"Mrs\" :        \"Mrs\",\n",
        "        \"Master\" :     \"Master\",\n",
        "        \"Mlle\":        \"Miss\",\n",
        "        \"Miss\" :       \"Miss\",\n",
        "        \"Capt\":        \"Officer\",\n",
        "        \"Col\":         \"Officer\",\n",
        "        \"Major\":       \"Officer\",\n",
        "        \"Dr\":          \"Officer\",\n",
        "        \"Rev\":         \"Officer\",\n",
        "        \"Jonkheer\":    \"Royalty\",\n",
        "        \"Don\":         \"Royalty\",\n",
        "        \"Sir\" :        \"Royalty\",\n",
        "        \"Countess\":    \"Royalty\",\n",
        "        \"Dona\":        \"Royalty\",\n",
        "        \"Lady\" :       \"Royalty\"\n",
        "    }\n",
        "    extracted_titles = df[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\n",
        "    df[\"Title\"] = extracted_titles.map(titles)\n",
        "    return df\n",
        "\n",
        "def create_dummies(df,column_name):\n",
        "    \"\"\"Create Dummy Columns (One Hot Encoding) from a single Column\n",
        "\n",
        "    Usage\n",
        "    ------\n",
        "\n",
        "    train = create_dummies(train,\"Age\")\n",
        "    \"\"\"\n",
        "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
        "    df = pd.concat([df,dummies],axis=1)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W9jIVSrSbL4w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pre_process(df):\n",
        "    df = process_ticket(df)\n",
        "    df = process_missing(df)\n",
        "    df = process_age(df)\n",
        "    df = process_fare(df)\n",
        "    df = process_titles(df)\n",
        "    df = process_cabin(df)\n",
        "\n",
        "    for col in [\"Age_categories\",\"Fare_categories\",\n",
        "                \"Title\",\"Cabin_type\",\"Sex\",\"Ticket\",\"Pclass\"]:\n",
        "        df = create_dummies(df,col)\n",
        "    \n",
        "    #df = df.drop([\"Age_categories\",\"Fare_categories\",\n",
        "                #\"Title\",\"Cabin_type\",\"Sex\",\"Ticket\"],axis=1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "all_data = pre_process(all_data)\n",
        "\n",
        "train = all_data.iloc[:891]\n",
        "train = pd.concat([train,survived],axis=1)\n",
        "holdout = all_data.iloc[891:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CvTOyioebL4z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4 - Exploring Data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WlNqa8AmbL4z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "explore_cols = [\"SibSp\",\"Parch\",\"Survived\"]\n",
        "explore = train[explore_cols].copy()\n",
        "explore.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4kekhl5qbL44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()\n",
        "\n",
        "explore.drop(\"Survived\",axis=1).plot.hist(alpha=0.5,bins=8)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "75z57Jx_bL47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "explore[\"familysize\"] = explore[[\"SibSp\",\"Parch\"]].sum(axis=1)\n",
        "explore.drop(\"Survived\",axis=1).plot.hist(alpha=0.5,bins=10)\n",
        "plt.xticks(range(11))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NrMEhJ2GbL5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "plt.clf()\n",
        "for col in explore.columns.drop(\"Survived\"):\n",
        "    pivot = explore.pivot_table(index=col,values=\"Survived\")\n",
        "    pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6HdONhP9bL5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The SibSp column shows the number of siblings and/or spouses each passenger had on board, while the Parch columns shows the number of parents or children each passenger had onboard. Neither column has any missing values.\n",
        "\n",
        "The distribution of values in both columns is skewed right, with the majority of values being zero.\n",
        "\n",
        "You can sum these two columns to explore the total number of family members each passenger had onboard. The shape of the distribution of values in this case is similar, however there are less values at zero, and the quantity tapers off less rapidly as the values increase.\n",
        "\n",
        "Looking at the survival rates of the the combined family members, you can see that few of the over 500 passengers with no family members survived, while greater numbers of passengers with family members survived."
      ]
    },
    {
      "metadata": {
        "id": "bAn_tn1bbL5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#5 - Engineering New Features\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "K9R33kK4bL5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_isalone(df):\n",
        "    df[\"familysize\"] = df[[\"SibSp\",\"Parch\"]].sum(axis=1)\n",
        "    df[\"isalone\"] = 0\n",
        "    df.loc[(df[\"familysize\"] == 0),\"isalone\"] = 1\n",
        "    #df = df.drop(\"familysize\",axis=1)\n",
        "    return df\n",
        "\n",
        "train = process_isalone(train)\n",
        "holdout = process_isalone(holdout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2OtjzNRbbL5K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#6 - Selecting the Best-Performing Features\n"
      ]
    },
    {
      "metadata": {
        "id": "C-2kSrkUbL5L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def select_features(df,index):\n",
        "    \n",
        "    # index\n",
        "    # 0 - random forest\n",
        "    # 1 - logistic regression\n",
        "    \n",
        "    # Remove non-numeric columns, columns that have null values\n",
        "    df = df.select_dtypes([np.number]).dropna(axis=1)\n",
        "    all_X = df.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
        "    all_y = df[\"Survived\"]\n",
        "    \n",
        "    clf_rf = RandomForestClassifier(random_state=1, n_estimators=100)\n",
        "    clf_lr = LogisticRegression()\n",
        "    clfs = [clf_rf,clf_lr]\n",
        "    \n",
        "    selector = RFECV(clfs[index],cv=10,n_jobs=-1)\n",
        "    selector.fit(all_X,all_y)\n",
        "    \n",
        "    best_columns = list(all_X.columns[selector.support_])\n",
        "    print(\"Best Columns \\n\"+\"-\"*12+\"\\n{}\\n\".format(best_columns))\n",
        "    \n",
        "    return best_columns\n",
        "\n",
        "cols_rf = select_features(train,0)\n",
        "cols_lr = select_features(train,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0KFlQjLJ6EAk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(cols_rf), cols_rf)\n",
        "print(len(cols_lr), cols_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AqooMKXdbL5N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#7 - Selecting and Tuning Different Algorithms\n"
      ]
    },
    {
      "metadata": {
        "id": "8uxMv_UxbL5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import numpy as np\n",
        "\n",
        "def select_model(df,features):\n",
        "    \n",
        "    all_X = df[features]\n",
        "    all_y = df[\"Survived\"]\n",
        "\n",
        "    # List of dictionaries, each containing a model name,\n",
        "    # it's estimator and a dict of hyperparameters\n",
        "    models = [\n",
        "        {\n",
        "            \"name\": \"LogisticRegression\",\n",
        "            \"estimator\": LogisticRegression(),\n",
        "            \"hyperparameters\":\n",
        "                {\n",
        "                    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n",
        "                }\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"KNeighborsClassifier\",\n",
        "            \"estimator\": KNeighborsClassifier(),\n",
        "            \"hyperparameters\":\n",
        "                {\n",
        "                    \"n_neighbors\": range(1,20,2),\n",
        "                    \"weights\": [\"distance\", \"uniform\"],\n",
        "                    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
        "                    \"p\": [1,2]\n",
        "                }\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"RandomForestClassifier\",\n",
        "            \"estimator\": RandomForestClassifier(random_state=1),\n",
        "            \"hyperparameters\":\n",
        "                {\n",
        "                    \"n_estimators\": [200],\n",
        "                    \"criterion\": [\"entropy\", \"gini\"],\n",
        "                    \"max_depth\": [10,20],\n",
        "                    \"max_features\": [\"log2\", \"sqrt\"],\n",
        "                    \"min_samples_leaf\": [1],\n",
        "                    \"min_samples_split\": [2]\n",
        "                }\n",
        "        },\n",
        "        {\n",
        "            \"name\":\"SVC\",\n",
        "            \"estimator\":SVC(),\n",
        "            \"hyperparameters\":\n",
        "                {\n",
        "                  \"kernel\": ['rbf'],  \n",
        "                  \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
        "                  \"gamma\": [0.001, 0.01, 0.1, 1]\n",
        "                }\n",
        "        },\n",
        "        {\n",
        "            # reference\n",
        "            # https://github.com/UltravioletAnalytics/kaggle-titanic/blob/master/sgdclassifier.py\n",
        "            \"name\":\"SGDC\",\n",
        "            \"estimator\": SGDClassifier(),\n",
        "            \"hyperparameters\":\n",
        "            {\n",
        "                \"loss\": [\"log\"],\n",
        "                \"alpha\": [0.001],\n",
        "                \"penalty\": [\"elasticnet\"],\n",
        "                \"l1_ratio\": [0.8],\n",
        "                \"shuffle\": [True],\n",
        "                \"learning_rate\": ['optimal'],\n",
        "                \"max_iter\":[1000]\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for model in models:\n",
        "        print(model['name'])\n",
        "        print('-'*len(model['name']))\n",
        "\n",
        "        grid = GridSearchCV(model[\"estimator\"],\n",
        "                            param_grid=model[\"hyperparameters\"],\n",
        "                            cv=10)\n",
        "        grid.fit(all_X,all_y)\n",
        "        model[\"best_params\"] = grid.best_params_\n",
        "        model[\"best_score\"] = grid.best_score_\n",
        "        model[\"best_model\"] = grid.best_estimator_\n",
        "\n",
        "        print(\"Best Score: {}\".format(model[\"best_score\"]))\n",
        "        print(\"Best Parameters: {}\\n\".format(model[\"best_params\"]))\n",
        "\n",
        "    return models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "LeDDIj4TbL5Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_a = select_model(train,cols_rf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-m8KwVTWbL5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_b = select_model(train,cols_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0L9BQt8zbL5W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#8 - Making a Submission to Kaggle\n"
      ]
    },
    {
      "metadata": {
        "id": "TgvGjgR1bL5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_submission_file(model,cols,filename):\n",
        "    holdout_data = holdout[cols]\n",
        "    predictions = model.predict(holdout_data)\n",
        "    \n",
        "    holdout_ids = holdout[\"PassengerId\"]\n",
        "    submission_df = {\"PassengerId\": holdout_ids,\n",
        "                 \"Survived\": predictions}\n",
        "    submission = pd.DataFrame(submission_df)\n",
        "\n",
        "    submission.to_csv(filename,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbc3u4DlbL5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_rf_model = result_b[3][\"best_model\"]\n",
        "save_submission_file(best_rf_model,cols_lr,\"submission_PreprocessingIvanovitch.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yum7k-wwbL5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#9 - Next Steps\n"
      ]
    },
    {
      "metadata": {
        "id": "0pNKdURxyJpK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We encourage you to continue working on this Kaggle competition. Here are some suggestions of next steps:\n",
        "\n",
        "- Continue to explore the data and create new features, following the workflow and using the functions we created.\n",
        "- Read more about the titanic and this Kaggle competition to get ideas for new features.\n",
        "- Use some different algorithms in the select_model() function, like [stochastic gradient descent](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) or [perceptron linear models](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html).\n",
        "- Experiment with [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) instead of **GridSearchCV** to speed up your **select_features()** function.\n",
        "\n",
        "Lastly, while the Titanic competition is great for learning about how to approach your first Kaggle competition, we recommend against spending many hours focused on trying to get to the top of the leaderboard. With such a small data set, there is a limit to how good your predictions can be, and your time would be better spent moving onto more complex competitions.\n",
        "\n",
        "Once you feel like you have a good understanding of the Kaggle workflow, you should look at some other competitions - a great next competition is the [House Prices Competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). A start point you can find [here](https://www.dataquest.io/blog/kaggle-getting-started/)."
      ]
    },
    {
      "metadata": {
        "id": "RbXYOTM635Ne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "jVsLwV7I3-s0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 10 - Loading the modified data"
      ]
    },
    {
      "metadata": {
        "id": "tA3Qf-lY33bY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_mod = pd.read_csv(\"./data/train_modified.csv\")\n",
        "holdout_mod = pd.read_csv(\"./data/holdout_modified.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MCPLIKej4YUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train_mod.info())\n",
        "train_mod.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uHRBLp2-4rBC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(holdout_mod.info())\n",
        "holdout_mod.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_pn2fkA6vIx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_mod = train_mod.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
        "y_mod = train_mod[\"Survived\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JH7ICe6T19xw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 11 - Using the  AdaBoostClassifier\n",
        "\n",
        "An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
        "\n",
        "This class implements the algorithm known as AdaBoost-SAMME [2].\n",
        "\n",
        "class **sklearn.ensemble.AdaBoostClassifier**(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None)[source]\n"
      ]
    },
    {
      "metadata": {
        "id": "3hBZSg3u0XP2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.externals.six.moves import zip\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_gaussian_quantiles\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mod, y_mod, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "bdt_real = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=15),\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1)\n",
        "\n",
        "bdt_discrete = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=15),\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1,\n",
        "    algorithm=\"SAMME\")\n",
        "\n",
        "bdt_real.fit(X_train, y_train)\n",
        "bdt_discrete.fit(X_train, y_train)\n",
        "\n",
        "real_test_errors = []\n",
        "discrete_test_errors = []\n",
        "\n",
        "for real_test_predict, discrete_train_predict in zip(\n",
        "        bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):\n",
        "    real_test_errors.append(\n",
        "        1. - accuracy_score(real_test_predict, y_test))\n",
        "    discrete_test_errors.append(\n",
        "        1. - accuracy_score(discrete_train_predict, y_test))\n",
        "\n",
        "n_trees_discrete = len(bdt_discrete)\n",
        "n_trees_real = len(bdt_real)\n",
        "\n",
        "# Boosting might terminate early, but the following arrays are always\n",
        "# n_estimators long. We crop them to the actual number of trees here:\n",
        "discrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]\n",
        "real_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]\n",
        "discrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.plot(range(1, n_trees_discrete + 1),\n",
        "         discrete_test_errors, c='black', label='SAMME')\n",
        "plt.plot(range(1, n_trees_real + 1),\n",
        "         real_test_errors, c='black',\n",
        "         linestyle='dashed', label='SAMME.R')\n",
        "plt.legend()\n",
        "plt.ylim(0.18, 0.62)\n",
        "plt.ylabel('Test Error')\n",
        "plt.xlabel('Number of Trees')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,\n",
        "         \"b\", label='SAMME', alpha=.5)\n",
        "plt.plot(range(1, n_trees_real + 1), real_estimator_errors,\n",
        "         \"r\", label='SAMME.R', alpha=.5)\n",
        "plt.legend()\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylim((.2,\n",
        "         max(real_estimator_errors.max(),\n",
        "             discrete_estimator_errors.max()) * 1.2))\n",
        "plt.xlim((-20, len(bdt_discrete) + 20))\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,\n",
        "         \"b\", label='SAMME')\n",
        "plt.legend()\n",
        "plt.ylabel('Weight')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylim((0, discrete_estimator_weights.max() * 1.2))\n",
        "plt.xlim((-20, n_trees_discrete + 20))\n",
        "\n",
        "# prevent overlapping y-axis labels\n",
        "plt.subplots_adjust(wspace=0.25)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovG_oVJpGDw9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bdt_real"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EOJH3eTAHRLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(real_test_errors)\n",
        "print(discrete_test_errors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KFM0JekZHleU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(bdt_real.score(X_test, y_test))\n",
        "print(bdt_discrete.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qyq6ysGvDGkD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make the prediction using the resulting model\n",
        "holdout_pred = holdout_mod.drop([\"PassengerId\"],axis=1)\n",
        "\n",
        "predictions = bdt_real.predict(holdout_pred)\n",
        "\n",
        "print(\"class = \", predictions)\n",
        "\n",
        "holdout_ids = holdout_mod[\"PassengerId\"]\n",
        "submission_df = {\"PassengerId\": holdout_ids, \"Survived\": predictions}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_AdaBoostClassifier_real_0.csv\",index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZvVgq1IhDZP0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make the prediction using the resulting model\n",
        "holdout_pred = holdout_mod.drop([\"PassengerId\"],axis=1)\n",
        "\n",
        "predictions = bdt_discrete.predict(holdout_pred)\n",
        "\n",
        "print(\"class = \", predictions)\n",
        "\n",
        "holdout_ids = holdout[\"PassengerId\"]\n",
        "submission_df = {\"PassengerId\": holdout_ids, \"Survived\": predictions}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_AdaBoostClassifier_discrete_0.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7DYF01EHT7wB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 11.1 Plot feature importance of AdaBoostClassifier Discrete"
      ]
    },
    {
      "metadata": {
        "id": "IxVBOU5sRtST",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# #############################################################################\n",
        "# Plot feature importance\n",
        "feature_importance = bdt_discrete.feature_importances_\n",
        "# make importances relative to max importance\n",
        "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos = np.arange(sorted_idx.shape[0]) + .5\n",
        "#plt.subplot(1, 2, 2)\n",
        "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "plt.yticks(pos, X_mod.columns[sorted_idx])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.title('Variable Importance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ew5va996cgvv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = pd.DataFrame()\n",
        "features['feature'] = X_mod.columns\n",
        "features['importance'] = bdt_discrete.feature_importances_\n",
        "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
        "features.set_index('feature', inplace=True)\n",
        "\n",
        "features.plot(kind='barh', figsize=(25, 25))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lU-_4XWKcl73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = pd.DataFrame()\n",
        "features['feature'] = X_mod.columns\n",
        "features['importance'] = bdt_real.feature_importances_\n",
        "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
        "features.set_index('feature', inplace=True)\n",
        "\n",
        "features.plot(kind='barh', figsize=(25, 25))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bkv813b9Rtwv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 11.2 Tunning the AdaBoostClassifier with RandomizedSearchCV \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DwfCHXJ3R1a_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# specify the training parameters \n",
        "#clf = AdaBoostClassifier(learning_rate=0.1, loss_function='Logloss', logging_level='Verbose')\n",
        "clf = AdaBoostClassifier()\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "#AdaBoostClassifier(algorithm='SAMME.R',\n",
        "#          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=15,\n",
        "#            max_features=None, max_leaf_nodes=None,\n",
        "#            min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#            min_samples_leaf=1, min_samples_split=2,\n",
        "#            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
        "#            splitter='best'),\n",
        "#          learning_rate=0.1, n_estimators=300, random_state=None)            \n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {#\"iterations\": sp_randint(15,50),\n",
        "              #\"max_depth\" : sp_randint(5,16),\n",
        "              \"base_estimator\" : [RandomForestClassifier(max_depth=10), RandomForestClassifier(max_depth=15) , DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=15, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') ],\n",
        "             \"learning_rate\" : uniform(0.1, 1),\n",
        "              \"n_estimators\" : sp_randint(10, 100),\n",
        "               \"algorithm\" : ['SAMME', 'SAMME.R']}\n",
        "\n",
        "# run randomized search \n",
        "\n",
        "#ValueError: Invalid parameter iterations for estimator AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
        "#          learning_rate=1.0, n_estimators=50, random_state=None). Check the list of available parameters with `estimator.get_params().keys()`.\n",
        "  \n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=5)\n",
        "\n",
        "start = time()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mod, y_mod, test_size=0.1, random_state=42)\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vK44DM5pVNnZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Sx_rJRTY2TO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random_search"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YeMImE-qVZxx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(random_search.score(X_mod, y_mod))\n",
        "print(random_search.score(X_train, y_train))\n",
        "print(random_search.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETO70PjPZIDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "holdout_mod = pd.read_csv(\"./data/holdout_modified.csv\")\n",
        "\n",
        "## 11.2 Tunning the AdaBoostClassifier with RandomizedSearchCV \n",
        "predictions = random_search.predict(holdout_mod.drop([\"PassengerId\"], axis = 1))\n",
        "\n",
        "holdout_ids = holdout[\"PassengerId\"]\n",
        "submission_df = {\"PassengerId\": holdout_ids, \"Survived\": predictions}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_AdaBoostClassifierRandomizedSearchCV0.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ajka81WT0cU0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Score of 0.73684**"
      ]
    },
    {
      "metadata": {
        "id": "EH7XcOPKz3LY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 12 - Using the CatBoost: Score of 0.80382\n",
        "\n",
        "\n",
        "Here we implements an Soluting using the CatBoost algorithm implementation.\n",
        "\n",
        "CatBoost is a machine learning algorithm that uses gradient boosting on decision trees:\n",
        "\n",
        "https://tech.yandex.com/catboost/doc/dg/concepts/python-quickstart-docpage/"
      ]
    },
    {
      "metadata": {
        "id": "QJja2NjTNjnP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 12.1 - Installing the CatBoost"
      ]
    },
    {
      "metadata": {
        "id": "iEn4dh5O20Qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_KFEfAzNqYv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 12.2 - Loading the modified data and executing the CatBoost"
      ]
    },
    {
      "metadata": {
        "id": "TxJjhI7GN4Aq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_mod = pd.read_csv(\"./data/train_modified.csv\")\n",
        "holdout_mod = pd.read_csv(\"./data/holdout_modified.csv\")\n",
        "X_mod = train_mod.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
        "y_mod = train_mod[\"Survived\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4mSH-xIU0Nhb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# specify the training parameters \n",
        "model = CatBoostClassifier(iterations=46, depth=10, learning_rate=0.1, loss_function='Logloss', logging_level='Verbose')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mod, y_mod, test_size=0.3, random_state=42)\n",
        "\n",
        "#train the model\n",
        "#model.fit(X_train, y_train)\n",
        "\n",
        "model.fit(X_mod, y_mod)\n",
        "\n",
        "score_train = model.score(X_train, y_train)\n",
        "print(\"Accuracy of Train Data\", score_train)\n",
        "\n",
        "score_test = model.score(X_test, y_test)\n",
        "print(\"Accuracy of Test Data\", score_test)\n",
        "\n",
        "# make the prediction using the resulting model\n",
        "predictions = model.predict(holdout_mod).astype(int)\n",
        "\n",
        "print(\"class = \", predictions)\n",
        "\n",
        "#preds_proba = model.predict_proba(test_data)\n",
        "#print(\"proba = \", preds_proba)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o4nFCDdgJ5rn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "holdout_ids = holdout[\"PassengerId\"]\n",
        "submission_df = {\"PassengerId\": holdout_ids, \"Survived\": predictions}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_CatBoostClassifier_1.csv\",index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T-7GjoCO25O1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Score of 0.80382**"
      ]
    },
    {
      "metadata": {
        "id": "1DzdMJCwL_SI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![Rank on the Kaggle](https://drive.google.com/uc?export=view&id=1N8GM-_16JgWArc_-v9vc5nmghRKaplLw)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "c1gExUwQmlBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  12.2.1 - Best result with CatBoostClassifier\n",
        "\n",
        "`CatBoostClassifier(iterations=20, depth=10, learning_rate=0.1, loss_function='Logloss', logging_level='Verbose')`\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mAjade07J77s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 12.3 - Using a Pool of CatBoost with Cross-validation"
      ]
    },
    {
      "metadata": {
        "id": "nF1AQmiw7u0g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from catboost import Pool, cv\n",
        "\n",
        "pool = Pool(X_mod, y_mod)\n",
        "\n",
        "params = {'iterations': 100, \n",
        "          'depth': 3,        \n",
        "          'loss_function': 'Logloss', \n",
        "          'verbose': False, \n",
        "          'roc_file': 'roc-file'}\n",
        "\n",
        "scores = cv(pool,params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "szxCdGcvFktZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h4twtrjnYVCB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 12.4 Tunning the CatBoost with RandomizedSearchCV: Bad Score of 0.74162\n"
      ]
    },
    {
      "metadata": {
        "id": "aLK5NUtRbMjS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from scipy.stats import randint as sp_randint\n",
        "\n",
        "np.linspace(0,1.5,16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vm0OPsa6aoP-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# specify the training parameters \n",
        "clf = CatBoostClassifier(loss_function='Logloss', logging_level='Verbose')\n",
        "\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {\"iterations\": sp_randint(15,50),\n",
        "              \"depth\" : sp_randint(5,16),\n",
        "              \"learning_rate\" : np.linspace(0,1,16)}\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=5)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X_mod, y_mod)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n",
        "# use a full grid over all parameters\n",
        "param_grid =  {\"iterations\": sp_randint(15,50),\n",
        "              \"depth\" : sp_randint(5,16),\n",
        "              \"learning_rate\" : uniform(0,1)}\n",
        "\n",
        "clf_grid = CatBoostClassifier(learning_rate=0.1, loss_function='Logloss', logging_level='Verbose')\n",
        "\n",
        "# run grid search\n",
        "#grid_search = GridSearchCV(clf_grid, param_grid=param_grid, cv=5)\n",
        "#start = time()\n",
        "#grid_search.fit(X_mod, y_mod)\n",
        "\n",
        "#print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
        "#      % (time() - start, len(grid_search.cv_results_['params'])))\n",
        "#report(grid_search.cv_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_33cUmShsdI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**The learning rate must be less than 1.**\n"
      ]
    },
    {
      "metadata": {
        "id": "sC_60DiWeITa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Random_search best_score:\", random_search.best_score_)\n",
        "#print(\"Grid_search best_score:\", grid_search.best_score_,\"\\n\")\n",
        "\n",
        "print(\"Random_search best_score:\", random_search.best_params_)\n",
        "#print(\"Grid_search best_score:\", grid_search.best_params_,\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wg4pKKWTfIQS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = random_search.predict(holdout_mod).astype(int)\n",
        "\n",
        "holdout_ids = holdout[\"PassengerId\"]\n",
        "submission_df = {\"PassengerId\": holdout_ids, \"Survived\": predictions}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_CatBoostClassifier_Random_Search.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f_LL14PblkzT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1n7LSwZz7UIHOiXAxsVfitxCX6VNJSyMS)\n"
      ]
    },
    {
      "metadata": {
        "id": "AhlDibc_mIln",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BAD! Overfit with: **\n",
        "\n",
        "**Random_search best_score: {'depth': 10, 'iterations': 46, 'learning_rate': 0.5} **"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Wq1OksGwnvGC"
      },
      "cell_type": "markdown",
      "source": [
        "# 13 - Appling the Voting strategy for Ensemble"
      ]
    },
    {
      "metadata": {
        "id": "kpQD6ge4ZJOf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_mod = pd.read_csv(\"./data/train_modified.csv\")\n",
        "holdout_mod = pd.read_csv(\"./data/holdout_modified.csv\")\n",
        "X_mod = train_mod.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
        "y_mod = train_mod[\"Survived\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mH1LQYryNbkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
        "from sklearn.metrics import make_scorer, roc_auc_score\n",
        "from scipy import stats\n",
        "\n",
        "auc = make_scorer(roc_auc_score)\n",
        "rand_list = {\"C\": stats.uniform(2, 10),\n",
        "                        \"gamma\": stats.uniform(0.1, 1)}\n",
        "\n",
        "\n",
        "# X_train0, X_test0, y_train0, y_test0 = train_test_split(X_mod, y_mod, test_size=0.1, random_state=42)\n",
        "#X = train[0::, 1::]\n",
        "#y = train[0::, 0]\n",
        "X = X_mod\n",
        "y = y_mod\n",
        "clf = SVC()\n",
        "\n",
        "\n",
        "rand_search = RandomizedSearchCV(clf, param_distributions = rand_list, n_iter = 20, n_jobs = 4, cv = 3, random_state = 2017, scoring = auc) \n",
        "rand_search.fit(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MXQKp6rzOHsZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rand_search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PyYZYsFuNcT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train0, X_test0, y_train0, y_test0 = train_test_split(X_mod, y_mod, test_size=0.1, random_state=42)\n",
        "\n",
        " #{'AdaBoostClassifier': 0.7950617283950618,\n",
        " #'CatBoostClassifier': 0.8185185185185185,\n",
        " #'DecisionTreeClassifier': 0.774074074074074,\n",
        " #'GaussianNB': 0.7358024691358025,\n",
        " #'GradientBoostingClassifier': 0.8160493827160493,\n",
        " #'KNeighborsClassifier': 0.7777777777777777,\n",
        " #'LinearDiscriminantAnalysis': 0.8135802469135802,\n",
        " #'LogisticRegression': 0.8209876543209876,\n",
        " #'RandomForestClassifier': 0.8246913580246913,\n",
        " #'SGDClassifier': 0.8185185185185185,\n",
        " #'SVC': 0.8160493827160493}\n",
        "\n",
        "  \n",
        " #{'algorithm': 'SAMME',\n",
        " # 'base_estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        " #            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
        "  #           min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "   #          min_samples_leaf=1, min_samples_split=2,\n",
        "   #          min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
        "    #         oob_score=False, random_state=None, verbose=0,\n",
        "     #        warm_start=False),\n",
        " #'learning_rate': 0.802656626129159,\n",
        " #'n_estimators': 36} \n",
        "\n",
        "\n",
        "classifiers = [\n",
        "    #KNeighborsClassifier(algorithm='brute', n_neighbors= 3, p= 1, weights='uniform'),\n",
        "    #SVC(probability=True, C= 1, gamma = 0.1, kernel= 'rbf'),\n",
        "    SVC(C=9.062264858625882, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.5565289675398223, kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False),\n",
        "    #DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(criterion= 'gini', max_depth=10, max_features= 'log2', min_samples_leaf= 1, min_samples_split= 2, n_estimators= 200),\n",
        "    SGDClassifier(alpha= 0.001, l1_ratio= 0.8, learning_rate='optimal', loss='log', max_iter= 1000, penalty= 'elasticnet', shuffle= True),\n",
        " \t  AdaBoostClassifier(RandomForestClassifier(max_depth=10,  criterion='gini', min_samples_leaf=1, min_samples_split=2), n_estimators=36, learning_rate=0.802656626129159, algorithm=\"SAMME\"),\n",
        "    GradientBoostingClassifier(),\n",
        "    #GaussianNB(),\n",
        "    LinearDiscriminantAnalysis(),\n",
        "    #QuadraticDiscriminantAnalysis(),\n",
        "    LogisticRegression(solver='newton-cg'),\n",
        "    CatBoostClassifier(iterations=20, depth=10, learning_rate=0.1, loss_function='Logloss', logging_level='Verbose')]\n",
        "\n",
        "log_cols = [\"Classifier\", \"Accuracy\"]\n",
        "log \t = pd.DataFrame(columns=log_cols)\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
        "\n",
        "X = X_mod\n",
        "y = y_mod\n",
        "\n",
        "acc_dict = {}\n",
        "\n",
        "for train_index, test_index in sss.split(X_mod, y_mod):\n",
        "\tX_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "\ty_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\t\n",
        "\tfor clf in classifiers:\n",
        "\t\tname = clf.__class__.__name__\n",
        "\t\tclf.fit(X_train, y_train)\n",
        "\t\ttrain_predictions = clf.predict(X_test)\n",
        "\t\tacc = accuracy_score(y_test, train_predictions)\n",
        "\t\tif name in acc_dict:\n",
        "\t\t\tacc_dict[name] += acc\n",
        "\t\telse:\n",
        "\t\t\tacc_dict[name] = acc\n",
        "\n",
        "for clf in acc_dict:\n",
        "\tacc_dict[clf] = acc_dict[clf] / 10.0\n",
        "\tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
        "\tlog = log.append(log_entry)\n",
        "\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Classifier Accuracy')\n",
        "\n",
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_elnGIo0W3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " acc_dict\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qn66dDHK0-kg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifiers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lOS-4mB1c6M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(X_train0)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined)\n",
        " \n",
        "acc = accuracy_score(y_train0, rounded)\n",
        "print(\"Predictions of X_train\", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbgHpEil6uu4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(X_test0)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined)\n",
        " \n",
        "acc = accuracy_score(y_test0, rounded)\n",
        "print(\"Predictions of X_test\", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bdg6RSz53m96",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test = pd.read_csv(\"./data/test.csv\")\n",
        "\n",
        "hold = holdout_mod.drop([\"PassengerId\"],axis=1)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(hold)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined).astype(int)\n",
        " \n",
        "holdout_ids = holdout[\"PassengerId\"]\n",
        "submission_df = {\"PassengerId\": holdout_ids, \"Survived\": rounded}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_Ensemble_Voting2.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i4SBJtOXVhCg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Score of 0.78947**"
      ]
    },
    {
      "metadata": {
        "id": "gSIa1aHuVhxX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 14 - VotingClassifier from Sklearn: Bad Score of 0.77990"
      ]
    },
    {
      "metadata": {
        "id": "O0V5-_DUVp6J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "classifiers = [\n",
        "    #KNeighborsClassifier(algorithm='brute', n_neighbors= 3, p= 1, weights='uniform'),\n",
        "    #SVC(probability=True, C= 1, gamma = 0.1, kernel= 'rbf'),\n",
        "    ('svm', SVC(C=9.062264858625882, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.5565289675398223, kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)),\n",
        "    #(DecisionTreeClassifier(),\n",
        "    ('rf', RandomForestClassifier(criterion= 'gini', max_depth=10, max_features= 'log2', min_samples_leaf= 1, min_samples_split= 2, n_estimators= 200)),\n",
        "    ('sg',SGDClassifier(alpha= 0.001, l1_ratio= 0.8, learning_rate='optimal', loss='log', max_iter= 1000, penalty= 'elasticnet', shuffle= True)),\n",
        " \t  #AdaBoostClassifier(RandomForestClassifier(max_depth=15), n_estimators=200, learning_rate=0.1, algorithm=\"SAMME\"),\n",
        "    ('gb',GradientBoostingClassifier()),\n",
        "    #GaussianNB()),\n",
        "    ('ld', LinearDiscriminantAnalysis()),\n",
        "    #QuadraticDiscriminantAnalysis()),\n",
        "    ('logr', LogisticRegression(solver='newton-cg')),\n",
        "    ('catb', CatBoostClassifier(iterations=20, depth=10, learning_rate=0.1, loss_function='Logloss', logging_level='Verbose'))]\n",
        "\n",
        "\n",
        "eclf1 = VotingClassifier(classifiers, voting='soft', flatten_transform=True)\n",
        "\n",
        "\n",
        "eclf1 = eclf1.fit(X_mod, y_mod)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5ZhbKsNXWCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eclf1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GsB--rPhygiM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(eclf1.score(X_mod, y_mod))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TlPcCW1c71r2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hold = holdout_mod.drop([\"PassengerId\"],axis=1)\n",
        "\n",
        "predictions = eclf1.predict(hold)\n",
        " \n",
        "holdout_ids = holdout[\"PassengerId\"]\n",
        "submission_df = {\"PassengerId\": holdout_ids, \"Survived\": predictions}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_VotingClassifier0.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6de7JbEsJnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1CpmCiebxXoQ-_B1eqNa9-GhYG2hqhUoe)\n"
      ]
    },
    {
      "metadata": {
        "id": "P3DJZpRLAuvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 15 - New Pre-processing with VotingClassifier from Sklearn: Bad Score of 0.77990"
      ]
    },
    {
      "metadata": {
        "id": "6wyQLrVyA0DD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re as re\n",
        "\n",
        "train = pd.read_csv('./data/train.csv', header=0, dtype={'Age': np.float64})\n",
        "test = pd.read_csv('./data/test.csv', header=0, dtype={'Age': np.float64})\n",
        "full_data = [train, test]\n",
        "\n",
        "### PRE-PROCESSING\n",
        "\n",
        "def get_title(name):\n",
        "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
        "    # If the title exists, extract and return it.\n",
        "    if title_search:\n",
        "        return title_search.group(1)\n",
        "    return \"\"\n",
        "\n",
        "for dataset in full_data:\n",
        "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
        "    dataset['IsAlone'] = 0\n",
        "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
        "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
        "    age_avg = dataset['Age'].mean()\n",
        "    age_std = dataset['Age'].std()\n",
        "    age_null_count = dataset['Age'].isnull().sum()\n",
        "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
        "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', \\\n",
        "                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    # Mapping Sex\n",
        "    dataset['Sex'] = dataset['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
        "\n",
        "    # Mapping titles\n",
        "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "\n",
        "    # Mapping Embarked\n",
        "    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n",
        "\n",
        "    # Mapping Fare\n",
        "    dataset.loc[dataset['Fare'] <= 10, 'Fare'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 10) & (dataset['Fare'] <= 20), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 20) & (dataset['Fare'] <= 30), 'Fare'] = 2\n",
        "    dataset.loc[dataset['Fare'] > 30, 'Fare'] = 3\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
        "\n",
        "    # Mapping Age\n",
        "    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n",
        "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
        "    dataset.loc[dataset['Age'] > 64, 'Age'] = 4\n",
        "\n",
        "train['CategoricalFare'] = pd.cut(train['Fare'], 4)\n",
        "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
        "\n",
        "# Feature Selection\n",
        "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n",
        "                 'Parch', 'FamilySize']\n",
        "train = train.drop(drop_elements, axis = 1)\n",
        "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
        "test = test.drop(drop_elements, axis = 1)\n",
        "#train = train.values\n",
        "#test = test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q7v-zIhrBbUk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "classifiers = [\n",
        "    #KNeighborsClassifier(algorithm='brute', n_neighbors= 3, p= 1, weights='uniform'),\n",
        "    #SVC(probability=True, C= 1, gamma = 0.1, kernel= 'rbf'),\n",
        "    ('svm', SVC(C=9.062264858625882, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.5565289675398223, kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)),\n",
        "    #(DecisionTreeClassifier(),\n",
        "    ('rf', RandomForestClassifier(criterion= 'gini', max_depth=10, max_features= 'log2', min_samples_leaf= 1, min_samples_split= 2, n_estimators= 200)),\n",
        "    ('sg',SGDClassifier(alpha= 0.001, l1_ratio= 0.8, learning_rate='optimal', loss='log', max_iter= 1000, penalty= 'elasticnet', shuffle= True)),\n",
        " \t  #AdaBoostClassifier(RandomForestClassifier(max_depth=15), n_estimators=200, learning_rate=0.1, algorithm=\"SAMME\"),\n",
        "    ('gb',GradientBoostingClassifier()),\n",
        "    #GaussianNB()),\n",
        "    ('ld', LinearDiscriminantAnalysis()),\n",
        "    #QuadraticDiscriminantAnalysis()),\n",
        "    ('logr', LogisticRegression(solver='newton-cg')),\n",
        "    ('catb', CatBoostClassifier(iterations=20, depth=10, learning_rate=0.1, loss_function='Logloss', logging_level='Verbose'))]\n",
        "\n",
        "\n",
        "eclf1 = VotingClassifier(classifiers, voting='soft', flatten_transform=True)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train0, X_test0, y_train0, y_test0 = train_test_split(X_mod, y_mod, test_size=0.1, random_state=42)\n",
        "\n",
        "eclf1 = eclf1.fit(train.drop([\"Survived\"], axis=1), train[\"Survived\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PM30lbcTC2ym",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(eclf1.score(train.drop([\"Survived\"], axis=1), train[\"Survived\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KjEEPTCwDkV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train0, X_test0, y_train0, y_test0 = train_test_split(train.drop([\"Survived\"], axis=1), train[\"Survived\"], test_size=0.1, random_state=42)\n",
        "\n",
        "#{'CatBoostClassifier': 0.8233333333333335,\n",
        "# 'GradientBoostingClassifier': 0.8188888888888888,\n",
        "# 'LinearDiscriminantAnalysis': 0.7933333333333332,\n",
        "# 'LogisticRegression': 0.7944444444444443,\n",
        "# 'RandomForestClassifier': 0.8088888888888889,\n",
        "# 'SGDClassifier': 0.7944444444444444,\n",
        "# 'SVC': 0.8122222222222222}\n",
        "\n",
        "classifiers = [\n",
        "    #KNeighborsClassifier(algorithm='brute', n_neighbors= 3, p= 1, weights='uniform'),\n",
        "    #SVC(probability=True, C= 1, gamma = 0.1, kernel= 'rbf'),\n",
        "    SVC(C=9.062264858625882, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.5565289675398223, kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False),\n",
        "    #DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(criterion= 'gini', max_depth=10, max_features= 'log2', min_samples_leaf= 1, min_samples_split= 2, n_estimators= 200),\n",
        "    #SGDClassifier(alpha= 0.001, l1_ratio= 0.8, learning_rate='optimal', loss='log', max_iter= 1000, penalty= 'elasticnet', shuffle= True),\n",
        " \t  AdaBoostClassifier(RandomForestClassifier(max_depth=10), n_estimators=48, learning_rate=1.0888, algorithm=\"SAMME.R\"),\n",
        "    GradientBoostingClassifier(),\n",
        "    #GaussianNB(),\n",
        "    #LinearDiscriminantAnalysis(),\n",
        "    #QuadraticDiscriminantAnalysis(),\n",
        "    #LogisticRegression(solver='newton-cg'),\n",
        "    CatBoostClassifier(iterations=20, depth=10, learning_rate=0.1, loss_function='Logloss', logging_level='Verbose')]\n",
        "\n",
        "log_cols = [\"Classifier\", \"Accuracy\"]\n",
        "log \t = pd.DataFrame(columns=log_cols)\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
        "\n",
        "X = train.drop([\"Survived\"], axis=1)\n",
        "y = train[\"Survived\"]\n",
        "\n",
        "acc_dict = {}\n",
        "\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "\tX_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "\ty_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\t\n",
        "\tfor clf in classifiers:\n",
        "\t\tname = clf.__class__.__name__\n",
        "\t\tclf.fit(X_train, y_train)\n",
        "\t\ttrain_predictions = clf.predict(X_test)\n",
        "\t\tacc = accuracy_score(y_test, train_predictions)\n",
        "\t\tif name in acc_dict:\n",
        "\t\t\tacc_dict[name] += acc\n",
        "\t\telse:\n",
        "\t\t\tacc_dict[name] = acc\n",
        "\n",
        "for clf in acc_dict:\n",
        "\tacc_dict[clf] = acc_dict[clf] / 10.0\n",
        "\tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
        "\tlog = log.append(log_entry)\n",
        "\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Classifier Accuracy')\n",
        "\n",
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BeHqC4-WD09n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(X)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined)\n",
        " \n",
        "acc = accuracy_score(y, rounded)\n",
        "print(\"Predictions of X_train\", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15_mjnp-EG3d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYXPdhK6Fxl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "hold = test\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(hold)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined).astype(int)\n",
        " \n",
        "holdout_ids = holdout[\"PassengerId\"]\n",
        "submission_df = {\"PassengerId\": holdout_ids, \"Survived\": rounded}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_Ensemble_Voting0_section15.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "poESFGvWrV4Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1jUT65euStJbzpF7eKdTHUzRcI_OyphxU)\n"
      ]
    },
    {
      "metadata": {
        "id": "D-EOGk2z30r0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 16 -  Another Preprocessing Try-all with KNN: Best Score of 0.83253\n",
        "\n",
        "\n",
        "Based on  https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83 \n"
      ]
    },
    {
      "metadata": {
        "id": "Hr3riqZC4ETd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NumPy\n",
        "import numpy as np\n",
        "\n",
        "# Dataframe operations\n",
        "import pandas as pd\n",
        "\n",
        "# Data visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scalers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression #logistic regression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn import svm #support vector Machine\n",
        "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
        "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
        "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
        "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
        "from sklearn.model_selection import train_test_split #training and testing data split\n",
        "from sklearn import metrics #accuracy measure\n",
        "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Cross-validation\n",
        "from sklearn.model_selection import KFold #for K-fold cross validation\n",
        "from sklearn.model_selection import cross_val_score #score evaluation\n",
        "from sklearn.model_selection import cross_val_predict #prediction\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#Common Model Algorithms\n",
        "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
        "\n",
        "#Common Model Helpers\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn import feature_selection\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "#Visualization\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "import seaborn as sns\n",
        "from pandas.tools.plotting import scatter_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iC2Sd_sG4HpO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading datasets\n"
      ]
    },
    {
      "metadata": {
        "id": "WFwyuews4JEr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"./data/train.csv\")\n",
        "test_df = pd.read_csv(\"./data/test.csv\")\n",
        "data_df = train_df.append(test_df) # The entire data: train + test."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J5YGXBas4Myq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Engineering features\n"
      ]
    },
    {
      "metadata": {
        "id": "NrowR2O94NvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_df['Title'] = data_df['Name']\n",
        "# Cleaning name and extracting Title\n",
        "for name_string in data_df['Name']:\n",
        "    data_df['Title'] = data_df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
        "\n",
        "# Replacing rare titles with more common ones\n",
        "mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n",
        "          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n",
        "data_df.replace({'Title': mapping}, inplace=True)\n",
        "titles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n",
        "for title in titles:\n",
        "    age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n",
        "    data_df.loc[(data_df['Age'].isnull()) & (data_df['Title'] == title), 'Age'] = age_to_impute\n",
        "    \n",
        "# Substituting Age values in TRAIN_DF and TEST_DF:\n",
        "train_df['Age'] = data_df['Age'][:891]\n",
        "test_df['Age'] = data_df['Age'][891:]\n",
        "\n",
        "# Dropping Title feature\n",
        "data_df.drop('Title', axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "data_df['Family_Size'] = data_df['Parch'] + data_df['SibSp']\n",
        "\n",
        "# Substituting Age values in TRAIN_DF and TEST_DF:\n",
        "train_df['Family_Size'] = data_df['Family_Size'][:891]\n",
        "test_df['Family_Size'] = data_df['Family_Size'][891:]\n",
        "\n",
        "\n",
        "data_df['Last_Name'] = data_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
        "data_df['Fare'].fillna(data_df['Fare'].mean(), inplace=True)\n",
        "\n",
        "DEFAULT_SURVIVAL_VALUE = 0.5\n",
        "data_df['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n",
        "\n",
        "for grp, grp_df in data_df[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n",
        "                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
        "    \n",
        "    if (len(grp_df) != 1):\n",
        "        # A Family group is found.\n",
        "        for ind, row in grp_df.iterrows():\n",
        "            smax = grp_df.drop(ind)['Survived'].max()\n",
        "            smin = grp_df.drop(ind)['Survived'].min()\n",
        "            passID = row['PassengerId']\n",
        "            if (smax == 1.0):\n",
        "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
        "            elif (smin==0.0):\n",
        "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
        "\n",
        "print(\"Number of passengers with family survival information:\", \n",
        "      data_df.loc[data_df['Family_Survival']!=0.5].shape[0])\n",
        "\n",
        "\n",
        "for _, grp_df in data_df.groupby('Ticket'):\n",
        "    if (len(grp_df) != 1):\n",
        "        for ind, row in grp_df.iterrows():\n",
        "            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
        "                smax = grp_df.drop(ind)['Survived'].max()\n",
        "                smin = grp_df.drop(ind)['Survived'].min()\n",
        "                passID = row['PassengerId']\n",
        "                if (smax == 1.0):\n",
        "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
        "                elif (smin==0.0):\n",
        "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
        "                        \n",
        "print(\"Number of passenger with family/group survival information: \" \n",
        "      +str(data_df[data_df['Family_Survival']!=0.5].shape[0]))\n",
        "\n",
        "# # Family_Survival in TRAIN_DF and TEST_DF:\n",
        "train_df['Family_Survival'] = data_df['Family_Survival'][:891]\n",
        "test_df['Family_Survival'] = data_df['Family_Survival'][891:]\n",
        "\n",
        "\n",
        "data_df['Fare'].fillna(data_df['Fare'].median(), inplace = True)\n",
        "\n",
        "# Making Bins\n",
        "data_df['FareBin'] = pd.qcut(data_df['Fare'], 5)\n",
        "\n",
        "label = LabelEncoder()\n",
        "data_df['FareBin_Code'] = label.fit_transform(data_df['FareBin'])\n",
        "\n",
        "train_df['FareBin_Code'] = data_df['FareBin_Code'][:891]\n",
        "test_df['FareBin_Code'] = data_df['FareBin_Code'][891:]\n",
        "\n",
        "train_df.drop(['Fare'], 1, inplace=True)\n",
        "test_df.drop(['Fare'], 1, inplace=True)\n",
        "\n",
        "\n",
        "data_df['AgeBin'] = pd.qcut(data_df['Age'], 4)\n",
        "\n",
        "label = LabelEncoder()\n",
        "data_df['AgeBin_Code'] = label.fit_transform(data_df['AgeBin'])\n",
        "\n",
        "train_df['AgeBin_Code'] = data_df['AgeBin_Code'][:891]\n",
        "test_df['AgeBin_Code'] = data_df['AgeBin_Code'][891:]\n",
        "\n",
        "train_df.drop(['Age'], 1, inplace=True)\n",
        "test_df.drop(['Age'], 1, inplace=True)\n",
        "\n",
        "\n",
        "train_df['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
        "test_df['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
        "\n",
        "train_df.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n",
        "               'Embarked'], axis = 1, inplace = True)\n",
        "test_df.drop(['Name','PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n",
        "              'Embarked'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "X = train_df.drop('Survived', 1)\n",
        "y = train_df['Survived']\n",
        "holdout = test_df.copy()\n",
        "\n",
        "\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "X = std_scaler.fit_transform(X)\n",
        "holdout = std_scaler.transform(holdout)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pH3jKRabofNN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 16.1 - Best Result of KNN: Score of 0.83253"
      ]
    },
    {
      "metadata": {
        "id": "I75ByiGWm8Gr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_neighbors = [6,7,8,9,10,11,12,14,16,18,20,22]\n",
        "algorithm = ['auto']\n",
        "weights = ['uniform', 'distance']\n",
        "leaf_size = list(range(1,50,5))\n",
        "hyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n",
        "               'n_neighbors': n_neighbors}\n",
        "gd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, \n",
        "                cv=10, scoring = \"roc_auc\")\n",
        "gd.fit(X, y)\n",
        "print(gd.best_score_)\n",
        "print(gd.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0apIMG724l6D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gd.best_estimator_.fit(X, y)\n",
        "y_pred = gd.best_estimator_.predict(holdout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VFo9zz1o4mFz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n",
        "                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n",
        "                           weights='uniform')\n",
        "knn.fit(X, y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yZFVJ9q54LZ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = knn.predict(holdout)\n",
        "\n",
        "temp = pd.DataFrame(pd.read_csv(\"./data/test.csv\")['PassengerId'])\n",
        "temp['Survived'] = y_pred\n",
        "temp.to_csv(\"./predictions/submission.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gDqUanFRkNQC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 16.1.1  - Submition on Kaggle with Best Result of KNN: Score of 0.83253"
      ]
    },
    {
      "metadata": {
        "id": "7KIeHJjQEBOV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1RZhyZPZAXsXh7Wx0sX-MTfmtIFr2KFv6)"
      ]
    },
    {
      "metadata": {
        "id": "cpQJ-O5xJ-12",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 16.2 - Another Preprocessing with VotingEnsemble"
      ]
    },
    {
      "metadata": {
        "id": "0lahfbprAs-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96PhrKKS6goa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train0, X_test0, y_train0, y_test0 = train_test_split(train.drop([\"Survived\"], axis=1), train[\"Survived\"], test_size=0.1, random_state=42)\n",
        "\n",
        "#{'CatBoostClassifier': 0.8233333333333335,\n",
        "# 'GradientBoostingClassifier': 0.8188888888888888,\n",
        "# 'LinearDiscriminantAnalysis': 0.7933333333333332,\n",
        "#'AdaBoostClassifier': 0.8377777777777776,\n",
        "# 'CatBoostClassifier': 0.8422222222222222,\n",
        "# 'GaussianNB': 0.7777777777777777,\n",
        "# 'KNeighborsClassifier': 0.8422222222222221,\n",
        "# 'SVC': 0.8133333333333332}\n",
        "# 'LogisticRegression': 0.7944444444444443,\n",
        "# 'RandomForestClassifier': 0.8088888888888889,\n",
        "# 'SGDClassifier': 0.7944444444444444,\n",
        "# 'SVC': 0.8122222222222222}\n",
        "\n",
        "classifiers = [\n",
        "    #KNeighborsClassifier(algorithm='brute', n_neighbors= 3, p= 1, weights='uniform'),\n",
        "    #SVC(probability=True, C= 1, gamma = 0.1, kernel= 'rbf'),\n",
        "    SVC(C=9.062264858625882, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.5565289675398223, kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False),\n",
        "    #DecisionTreeClassifier(),\n",
        "    #RandomForestClassifier(criterion= 'gini', max_depth=10, max_features= 'log2', min_samples_leaf= 1, min_samples_split= 2, n_estimators= 200),\n",
        "    #SGDClassifier(alpha= 0.001, l1_ratio= 0.8, learning_rate='optimal', loss='log', max_iter= 1000, penalty= 'elasticnet', shuffle= True),\n",
        " \t  AdaBoostClassifier(RandomForestClassifier(max_depth=6), n_estimators=48, learning_rate=1.0888, algorithm=\"SAMME.R\"),\n",
        "    #GradientBoostingClassifier(),\n",
        "    #GaussianNB(),\n",
        "    #LinearDiscriminantAnalysis(),\n",
        "    #QuadraticDiscriminantAnalysis(),\n",
        "    LogisticRegression(solver='newton-cg'),\n",
        "    CatBoostClassifier(iterations=46, depth=6, learning_rate=0.1, loss_function='Logloss', logging_level='Verbose'),\n",
        "    KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n",
        "                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n",
        "                           weights='uniform')]\n",
        "\n",
        "log_cols = [\"Classifier\", \"Accuracy\"]\n",
        "log \t = pd.DataFrame(columns=log_cols)\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
        "\n",
        "#X = train.drop([\"Survived\"], axis=1)\n",
        "#y = train[\"Survived\"]\n",
        "\n",
        "acc_dict = {}\n",
        "\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "\tX_train, X_test = X[train_index], X[test_index]\n",
        "\ty_train, y_test = y[train_index], y[test_index]\n",
        "\t\n",
        "\tfor clf in classifiers:\n",
        "\t\tname = clf.__class__.__name__\n",
        "\t\tclf.fit(X_train, y_train)\n",
        "\t\ttrain_predictions = clf.predict(X_test)\n",
        "\t\tacc = accuracy_score(y_test, train_predictions)\n",
        "\t\tif name in acc_dict:\n",
        "\t\t\tacc_dict[name] += acc\n",
        "\t\telse:\n",
        "\t\t\tacc_dict[name] = acc\n",
        "\n",
        "for clf in acc_dict:\n",
        "\tacc_dict[clf] = acc_dict[clf] / 10.0\n",
        "\tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
        "\tlog = log.append(log_entry)\n",
        "\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Classifier Accuracy')\n",
        "\n",
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kYPsqWBUAnpk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Mw7TdaIhAnph",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(X)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined)\n",
        " \n",
        "acc = accuracy_score(y, rounded)\n",
        "print(\"Predictions of X_train\", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A6cREGqHAnpn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(test_df)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined).astype(int)\n",
        " \n",
        "temp = pd.read_csv(\"./data/test.csv\")\n",
        "\n",
        "submission_df = {\"PassengerId\": temp['PassengerId'], \"Survived\": rounded}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_Ensemble_Voting1_section16.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FM58-jWX1LWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Score of 0.76076**"
      ]
    },
    {
      "metadata": {
        "id": "--Jn-dm7a3Mk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 16.2 - With XGBoost: Score of 0.80861\n"
      ]
    },
    {
      "metadata": {
        "id": "egKvrUO7J0wA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn"
      ]
    },
    {
      "metadata": {
        "id": "T7VsLenGbVGs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmeI5pGibFbd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "\n",
        "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
        "\n",
        "import xgboost as xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUgXrc54di07",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6iYvhwq0eJJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def report_best_scores(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K49hAv5QbYMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
        "xgb_model.fit(X, y)\n",
        "\n",
        "y_pred = xgb_model.predict(X)\n",
        "\n",
        "print(confusion_matrix(y, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kD36qM70b0CV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = []\n",
        "\n",
        "for train_index, test_index in kfold.split(X):   \n",
        "    X_train0, X_test0= X[train_index], X[test_index]\n",
        "    y_train0, y_test0 = y[train_index], y[test_index]\n",
        "\n",
        "    xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
        "    xgb_model.fit(X_train0, y_train0)\n",
        "    \n",
        "    y_pred = xgb_model.predict(X_test0)\n",
        "    \n",
        "    scores.append(accuracy_score(y_test0, y_pred0))\n",
        "    \n",
        "display_scores(np.sqrt(scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9BHb8l6cXg6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Early stopping**"
      ]
    },
    {
      "metadata": {
        "id": "WWavJ-FGcWKl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# if more than one evaluation metric are given the last one is used for early stopping\n",
        "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n",
        "\n",
        "X_train0, X_test0, y_train0, y_test0 = train_test_split(X, y, random_state=42)\n",
        "\n",
        "xgb_model.fit(X_train0, y_train0, early_stopping_rounds=5, eval_set=[(X_test0, y_test0)])\n",
        "\n",
        "y_pred0 = xgb_model.predict(X_test0)\n",
        "\n",
        "accuracy_score(y_test0, y_pred0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kzeszJIrb9qk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Searching**"
      ]
    },
    {
      "metadata": {
        "id": "p4ZE93AwcFkH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "params = {\n",
        "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
        "    \"gamma\": uniform(0, 0.5),\n",
        "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
        "    \"max_depth\": randint(2, 6), # default 3\n",
        "    \"n_estimators\": randint(100, 150), # default 100\n",
        "    \"subsample\": uniform(0.6, 0.4)\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n",
        "\n",
        "search.fit(X, y)\n",
        "\n",
        "report_best_scores(search.cv_results_, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ZvLBkJdcecb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"best score: {0}, best iteration: {1}, best ntree limit {2}\".format(xgb_model.best_score, xgb_model.best_iteration, xgb_model.best_ntree_limit))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AN5uWHlCh0Z3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "search"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x80A4fqDerEP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "search.score(X_test0, y_test0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bKawsM1-dt6_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = search.predict(holdout)\n",
        "\n",
        "temp = pd.DataFrame(pd.read_csv(\"./data/test.csv\")['PassengerId'])\n",
        "temp['Survived'] = y_pred\n",
        "temp.to_csv(\"./predictions/submission_XGBoost_RandomSearch0.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9P-eIwiu3Uwn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Score of 0.80861**"
      ]
    },
    {
      "metadata": {
        "id": "SVRDrrVniPQM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 16.2.1  - Submition on Kaggle with Best Result of XGBoost: Score of 0.80861"
      ]
    },
    {
      "metadata": {
        "id": "jGjiO5DvicZo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1KHk2xDOt8BvN8EHXu1nu6-sWop1ZvqSs)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "H-l88uo7h7hj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 16.3  - Best Results: KNN, CATBoost and XGBoost"
      ]
    },
    {
      "metadata": {
        "id": "zf-ylMI6iM3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train0, X_test0, y_train0, y_test0 = train_test_split(train.drop([\"Survived\"], axis=1), train[\"Survived\"], test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "classifiers = [\n",
        "    xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
        "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
        "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
        "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "       silent=True, subsample=1),\n",
        "    CatBoostClassifier(iterations=46, depth=6, learning_rate=0.1, loss_function='Logloss', logging_level='Verbose'),\n",
        "    KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n",
        "                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n",
        "                           weights='uniform')]\n",
        "\n",
        "log_cols = [\"Classifier\", \"Accuracy\"]\n",
        "log \t = pd.DataFrame(columns=log_cols)\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
        "\n",
        "#X = train.drop([\"Survived\"], axis=1)\n",
        "#y = train[\"Survived\"]\n",
        "\n",
        "acc_dict = {}\n",
        "\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "\tX_train, X_test = X[train_index], X[test_index]\n",
        "\ty_train, y_test = y[train_index], y[test_index]\n",
        "\t\n",
        "\tfor clf in classifiers:\n",
        "\t\tname = clf.__class__.__name__\n",
        "\t\tclf.fit(X_train, y_train)\n",
        "\t\ttrain_predictions = clf.predict(X_test)\n",
        "\t\tacc = accuracy_score(y_test, train_predictions)\n",
        "\t\tif name in acc_dict:\n",
        "\t\t\tacc_dict[name] += acc\n",
        "\t\telse:\n",
        "\t\t\tacc_dict[name] = acc\n",
        "\n",
        "for clf in acc_dict:\n",
        "\tacc_dict[clf] = acc_dict[clf] / 10.0\n",
        "\tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
        "\tlog = log.append(log_entry)\n",
        "\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Classifier Accuracy')\n",
        "\n",
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJiNAdTqlLkq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jOKZ3qfVlnCW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(X)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined)\n",
        " \n",
        "acc = accuracy_score(y, rounded)\n",
        "print(\"Predictions of X_train\", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "m6hVGZG3lnCd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for clf in classifiers:\n",
        "  predictions.append(clf.predict_proba(holdout)[:,1])\n",
        "    \n",
        "combined = np.sum(predictions, axis=0) / len(classifiers)     \n",
        "rounded = np.round(combined).astype(int)\n",
        " \n",
        "temp = pd.read_csv(\"./data/test.csv\")\n",
        "\n",
        "submission_df = {\"PassengerId\": temp['PassengerId'], \"Survived\": rounded}\n",
        "submission = pd.DataFrame(submission_df)\n",
        "submission.to_csv(\"./predictions/submission_Voting_CatBoost_KNN_XGB.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNsI5CTKovzI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 16.3.1 Submition on Kaggle with Voting: score of 0.79425\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1FD9RVt-RM-lfAR0ynVfOmcMO9mCzX4rk)\n",
        "\n"
      ]
    }
  ]
}